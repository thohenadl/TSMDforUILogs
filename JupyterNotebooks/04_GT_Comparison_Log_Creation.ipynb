{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b814bb64",
   "metadata": {},
   "source": [
    "# Notebook to generate an extended UI Log\n",
    "\n",
    "Based on the Student Record (SR) and Reimbursement (RT) logs from Leno et al., this notebook generates an extended version of the log.\n",
    "\n",
    "The logs are in the folder called \"Leno\".\n",
    "\n",
    "Properties of the original logs:\n",
    "1. SR_RT_joint: Containing all Student Record traces and afterwards all Reimbursement traces.\n",
    "2. ST_RT_paarallel: Contains all Student Records traces alternating with all Reimbursement traces.\n",
    "\n",
    "Gathering of the original Leno data from https://figshare.com/articles/dataset/UI_logs/12543587\n",
    "\n",
    "Properties of the Extended Logs:\n",
    "1. Extended_SR_RT_joint: Between all traces there are X randomly generated events. X can be set in this notebook.\n",
    "2. Extended_ST_RT_parallel: Same as for Extended_SR_RT_joint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a8ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '../logs/Leno/'\n",
    "srrt_plus_filename = \"SR_RT_plus\"\n",
    "srrt_parallel_filename = \"SR_RT_parallel\"\n",
    "\n",
    "list_of_logs = [srrt_plus_filename, srrt_parallel_filename,]\n",
    "\n",
    "text_encoding_method = \"utf-8\"\n",
    "seperator = \";\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5bd88",
   "metadata": {},
   "source": [
    "#### Execution of Log Generation for Discovery\n",
    "\n",
    "Generating two extended logs\n",
    "1. Adding a case id for all existing cases\n",
    "2. Adding 50 random actions between all cases to simulate long time recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc6663b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_rows(df: pd.DataFrame, num_rows: int) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generates a list of random rows from existing DataFrame values, and adds random data for URL,\n",
    "    content, and target.workbookName columns.\n",
    "    \"\"\"\n",
    "    columns_to_shuffle = [col for col in df.columns if col != \"caseid\"]\n",
    "    \n",
    "    # Predefined lists for random data\n",
    "    random_actions = [\n",
    "        \"copyCell\", \"paste\", \"editField\", \"clickButton\", \"clickLink\", \"selectWorksheet\",\n",
    "        \"copyRange\", \"form_submit\", \"createNewTab\"\n",
    "    ]\n",
    "\n",
    "    random_urls = [\n",
    "        \"https://example.com\", \"https://example.org\", \"https://example.net\", \"\", \" \", \n",
    "        \"https://randomsite.com\", \"https://testsite.com\", \"https://anotherurl.com\", \n",
    "        \"https://sap.example.com\", \"https://service-now.example.com\", \"https://salesforce.example.com\", \n",
    "        \"https://jira.example.com\", \"https://confluence.example.com\", \"https://microsoft.com\", \n",
    "        \"https://office365.example.com\", \"https://slack.com\", \"https://zoom.us\", \n",
    "        \"https://google.com\", \"https://github.com\", \"https://linkedin.com\", \n",
    "        \"https://workday.example.com\", \"https://oracle.com\", \"https://adobe.com\"\n",
    "    ]\n",
    "    random_content = [\n",
    "        \"Lorem ipsum dolor sit amet.\", \"This is a random content example.\", \n",
    "        \"Random text for data generation.\", \"\", \" \", \"Sample content for testing.\", \n",
    "        \"Another random string.\", \"Test content for the application.\", \n",
    "        \"Placeholder text for demonstration.\", \"Randomly generated content.\"\n",
    "    ]\n",
    "    random_file_names = [\n",
    "        \"report_final.xlsx\", \"data_analysis.csv\", \"project_notes.pdf\", \"budget_2023.xlsx\",\n",
    "        \"presentation.csv\", \"meeting_minutes.xlsx\", \"research_paper.xml\"\n",
    "    ]\n",
    "    random_cell_names = [\n",
    "        \"A1\", \"B2\", \"C3\", \"D4\", \"E5\", \"F6\", \"G7\", \"H8\", \"I9\", \"J10\", \"K11\", \"L12\",\n",
    "        \"M13\", \"N14\", \"O15\", \"P16\", \"Q17\", \"R18\", \"S19\", \"T20\", \"U21\", \"V22\", \"W23\",\n",
    "        \"X24\", \"Y25\", \"Z26\", \"AA27\", \"AB28\", \"AC29\", \"AD30\", \"AE31\", \"AF32\", \"AG33\",\n",
    "    ]\n",
    "    random_html_tags = [\n",
    "        \"div\", \"span\", \"p\", \"a\", \"img\", \"h5\", \"h6\", \"ul\", \"ol\", \"li\",\n",
    "        \"table\", \"form\", \"input\", \"button\", \"select\", \"option\", \"textarea\", \"label\"\n",
    "    ]\n",
    "    random_sheet_names = [\n",
    "        \"Sheet1\", \"Sheet2\", \"Sheet3\", \"Sheet4\", \"Sheet5\", \"Sheet6\", \"Sheet7\"\n",
    "    ]\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for _ in range(num_rows):\n",
    "        random_row = {col: random.choice(df[col].tolist()) for col in columns_to_shuffle}\n",
    "\n",
    "        # fixed values\n",
    "        random_row[\"eventType\"] = random.choice(random_actions)\n",
    "        random_row[\"content\"] = random.choice(random_content)\n",
    "\n",
    "        if random_row[\"targetApp\"] == \"Chrome\":\n",
    "            random_row[\"target.sheetName\"] = \"\"\n",
    "            random_row[\"target.workbookName\"] = \"\"\n",
    "            random_row[\"url\"] = random.choice(random_urls)\n",
    "            random_row[\"target.id\"] = random.choice(random_html_tags)\n",
    "\n",
    "        elif random_row[\"targetApp\"] == \"Excel\":\n",
    "            random_row[\"target.id\"] = random.choice(random_cell_names)\n",
    "            random_row[\"target.workbookName\"] = random.choice(random_file_names)\n",
    "            random_row[\"target.sheetName\"] = random.choice(random_sheet_names)\n",
    "            random_row[\"target.tagName\"] = \"\"\n",
    "            random_row[\"url\"] = \"\"\n",
    "            random_row[\"target.type\"] = \"\"\n",
    "            random_row[\"target.href\"] = \"\"\n",
    "\n",
    "        rows.append(random_row)\n",
    "                                            \n",
    "    return pd.DataFrame(rows)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f68b9",
   "metadata": {},
   "source": [
    "### Ground Truth Calculation for unextended Log\n",
    "\n",
    "Due to incomplete routines: The last SR and the first RT Routine do not match in the PLUS log. Have to be adjusted manually as automating this is not feasible.\n",
    "Same for the SRRT Parallel Log: The last ground truth values have to be adjusted to match the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e47de55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_filename in list_of_logs:\n",
    "    df = pd.read_csv(file_path + log_filename + \".csv\", encoding=text_encoding_method, sep=seperator)\n",
    "    df[\"caseid\"] = -1\n",
    "    cond = (df[\"url\"] == \"https://forms.zoho.com/universityofmelbourne/form/NewRecord/thankyou\") | ((df[\"url\"] == \"https://submit.jotform.com/submit/200477494954062/\") & (df[\"eventType\"] == \"clickLink\"))\n",
    "    insert_positions = df.index[cond].tolist()\n",
    "    blocks = []\n",
    "    og_block_nr = 1\n",
    "    current = 0\n",
    "    current_out_index = 0\n",
    "    gt_df = pd.DataFrame(columns=[\"caseid\",\"start_index\",\"length\",\"motif\"])\n",
    "    for pos in insert_positions:\n",
    "        length = pos - current\n",
    "        motif = \"RT\" if length > 40 else \"SR\"\n",
    "        gt_new_row = {\n",
    "            \"caseid\": og_block_nr,\n",
    "            \"start_index\": current_out_index,\n",
    "            \"length\": length,\n",
    "            \"motif\": motif\n",
    "        }\n",
    "        gt_df = pd.concat([gt_df, pd.DataFrame([gt_new_row])], ignore_index=True)\n",
    "        og_block_nr += 1\n",
    "        if current_out_index == 0:\n",
    "            current_out_index += length +1 \n",
    "        else:\n",
    "            current_out_index += length\n",
    "        current = pos\n",
    "\n",
    "    # Adding Case IDs >> Important >> Some manual changes will be necessary after this step to ensure correctness, because the rule under cond does not guarantee perfect splits.\n",
    "    df[\"caseid\"] = 0\n",
    "    for i,gt in gt_df.iterrows():\n",
    "        start = gt[\"start_index\"]\n",
    "        df.loc[start:, \"caseid\"] = i + 1\n",
    "    \n",
    "    df.to_csv(file_path + \"202511_\" + log_filename + \".csv\", index=False, sep=seperator, encoding=text_encoding_method)\n",
    "    gt_df.to_csv(file_path + \"202511_\" + log_filename + \"_ground_truth.csv\", index=False, sep=seperator, encoding=text_encoding_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4817c39",
   "metadata": {},
   "source": [
    "### Creation of extended Log and Calculation of Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "498970a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_filename in list_of_logs:\n",
    "    df = pd.read_csv(file_path + log_filename + \".csv\", encoding=text_encoding_method, sep=seperator)\n",
    "    df[\"caseid\"] = -1\n",
    "    cond = (df[\"url\"] == \"https://forms.zoho.com/universityofmelbourne/form/NewRecord/thankyou\") | ((df[\"url\"] == \"https://submit.jotform.com/submit/200477494954062/\") & (df[\"eventType\"] == \"clickLink\"))\n",
    "    insert_positions = df.index[cond].tolist()\n",
    "    blocks = []\n",
    "    og_block_nr = 1\n",
    "    current = 0\n",
    "    current_out_index = 0\n",
    "    num_of_inserts = 50\n",
    "    gt_df = pd.DataFrame(columns=[\"caseid\",\"start_index\",\"length\",\"motif\"])\n",
    "    for pos in insert_positions:\n",
    "        block = df.iloc[current:pos+1].copy()\n",
    "        block[\"caseid\"] = og_block_nr\n",
    "        blocks.append(block)\n",
    "\n",
    "        rand = generate_random_rows(df, num_of_inserts).copy()\n",
    "        blocks.append(rand)\n",
    "\n",
    "        length = pos - current +1\n",
    "        motif = \"RT\" if length > 40 else \"SR\"\n",
    "\n",
    "        gt_new_row = {\n",
    "            \"caseid\": og_block_nr,\n",
    "            \"start_index\": current_out_index,\n",
    "            \"length\": length,\n",
    "            \"motif\": motif\n",
    "        }\n",
    "\n",
    "        gt_df = pd.concat([gt_df, pd.DataFrame([gt_new_row])], ignore_index=True)\n",
    "\n",
    "        og_block_nr += 1\n",
    "        current_out_index += length + num_of_inserts\n",
    "        current = pos+1\n",
    "\n",
    "    blocks.append(df.iloc[current:])  # tail\n",
    "\n",
    "    out = pd.concat(blocks, ignore_index=True)\n",
    "    out.to_csv(file_path + \"202511_\" + log_filename + \"_extended.csv\", index=False, sep=seperator, encoding=text_encoding_method)\n",
    "    gt_df.to_csv(file_path + \"202511_\" + log_filename + \"_extended_ground_truth.csv\", index=False, sep=seperator, encoding=text_encoding_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ec325",
   "metadata": {},
   "source": [
    "### Annotate the rows with caseId, Zeros for Noise and Task Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_9340\\4046519895.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'StudentRecord' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_plus.loc[df_plus[\"caseid\"]  >= 1, \"Task\"] = \"StudentRecord\"\n",
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_9340\\4046519895.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'StudentRecord' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_parallel.loc[df_parallel[\"caseid\"] % 2 >= 1, \"Task\"] = \"StudentRecord\"\n",
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_9340\\4046519895.py:22: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'StudentRecord' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_plus_ext.loc[df_plus_ext[\"caseid\"]  >= 1, \"Task\"] = \"StudentRecord\"\n",
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_9340\\4046519895.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'StudentRecord' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_parallel_ext.loc[df_parallel_ext[\"caseid\"] % 2 >= 1, \"Task\"] = \"StudentRecord\"\n"
     ]
    }
   ],
   "source": [
    "df_plus = pd.read_csv(file_path + \"202511_SR_RT_plus.csv\", sep=\";\")\n",
    "df_parallel = pd.read_csv(file_path + \"202511_SR_RT_parallel.csv\", sep=\";\")\n",
    "df_plus_ext = pd.read_csv(file_path +\"202511_SR_RT_plus_extended.csv\", sep=\";\")\n",
    "df_parallel_ext = pd.read_csv(file_path + \"202511_SR_RT_parallel_extended.csv\", sep=\";\")\n",
    "\n",
    "df_plus[\"Task\"] = np.nan\n",
    "df_parallel[\"Task\"] = np.nan\n",
    "df_plus.loc[df_plus[\"caseid\"]  >= 1, \"Task\"] = \"StudentRecord\"\n",
    "df_plus.loc[df_plus[\"caseid\"] >50 , \"Task\"] = \"Reimbursement\"\n",
    "df_plus.loc[df_plus[\"caseid\"] == 0, \"Task\"] = \"\"\n",
    "df_plus.rename(columns={\"caseid\":\"idx\"}, inplace=True)\n",
    "df_plus.to_csv(file_path + \"202511_SR_RT_plus_labeled4Rebmann.csv\", sep=\",\", index=False)\n",
    "\n",
    "df_parallel.loc[df_parallel[\"caseid\"] % 2 >= 1, \"Task\"] = \"StudentRecord\"\n",
    "df_parallel.loc[df_parallel[\"Task\"].isna() , \"Task\"] = \"Reimbursement\"\n",
    "df_parallel.loc[df_parallel[\"caseid\"].isna(), \"Task\"] = \"\"\n",
    "df_parallel.rename(columns={\"caseid\":\"idx\"}, inplace=True)\n",
    "df_parallel.to_csv(file_path + \"202511_SR_RT_parallel_labeled4Rebmann.csv\", sep=\",\", index=False)\n",
    "\n",
    "df_plus_ext[\"Task\"] = \"\"\n",
    "df_parallel_ext[\"Task\"] = \"\"\n",
    "df_plus_ext.loc[df_plus_ext[\"caseid\"] >= 1, \"Task\"] = \"StudentRecord\"\n",
    "df_plus_ext.loc[df_plus_ext[\"caseid\"] >50 , \"Task\"] = \"Reimbursement\"\n",
    "df_plus_ext.loc[df_plus_ext[\"caseid\"].isna(), \"caseid\"] = 0\n",
    "df_plus_ext.loc[df_plus_ext[\"caseid\"] == 0, \"Task\"] = \"Noise\"\n",
    "df_plus_ext.rename(columns={\"caseid\":\"idx\"}, inplace=True)\n",
    "df_plus_ext[\"idx\"] = pd.to_numeric(df_plus_ext[\"idx\"])\n",
    "df_plus_ext.to_csv(file_path + \"202511_SR_RT_plus_extended_labeled4Rebmann.csv\", sep=\",\", index=False)\n",
    "\n",
    "df_parallel_ext.loc[df_parallel_ext[\"caseid\"] % 2 >= 1, \"Task\"] = \"StudentRecord\"\n",
    "df_parallel_ext.loc[df_parallel_ext[\"Task\"].isna() , \"Task\"] = \"Reimbursement\"\n",
    "df_parallel_ext.loc[df_parallel_ext[\"caseid\"].isna(), \"caseid\"] = 0\n",
    "df_parallel_ext.loc[df_parallel_ext[\"caseid\"] == 0, \"Task\"] = \"Noise\"\n",
    "df_parallel_ext.rename(columns={\"caseid\":\"idx\"}, inplace=True)\n",
    "df_parallel_ext.to_csv(file_path + \"202511_SR_RT_parallel_extended_labeled4Rebmann.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80109e",
   "metadata": {},
   "source": [
    "### Create Ground Truth for Leno Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "453c48e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202511_SR_RT_plus.csv\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "202511_SR_RT_parallel.csv\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "202511_SR_RT_plus_extended.csv\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "202511_SR_RT_parallel_extended.csv\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n",
      "Duplicate Found, skipping...\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# Folder path in which the UI logs are which should be transformed into RPM Segmentor ground truth or SmartRPA segmentor files\n",
    "folder_path = \"../logs/Leno/\"\n",
    "\n",
    "# Settings for Leno Logs generated for experiment\n",
    "logs = [\"202511_SR_RT_plus.csv\",\"202511_SR_RT_parallel.csv\",\"202511_SR_RT_plus_extended.csv\",\"202511_SR_RT_parallel_extended.csv\"]\n",
    "gts = [\"202511_SR_RT_plus_ground_truth.csv\",\"202511_SR_RT_parallel_ground_truth.csv\",\"202511_SR_RT_plus_extended_ground_truth.csv\",\"202511_SR_RT_parallel_extended_ground_truth.csv\"]\n",
    "\n",
    "seperator = \";\" # \",\" for SmartRPA, \";\" for Tockler/AWT\n",
    "encoding_method = \"utf-8\" # UTF-8 for SmartRPA, latin-1 for Tockler/AWT\n",
    "\n",
    "# Read Files 1. Log 2. Validation Data to identify patterns\n",
    "for i,log in enumerate(logs):\n",
    "    print(log)\n",
    "    log_filename = log\n",
    "    log = pd.read_csv(folder_path + log_filename, encoding=encoding_method, sep=seperator)\n",
    "    validation_data = pd.read_csv(folder_path + gts[i], encoding=encoding_method, sep=seperator)\n",
    "    # Get Index of Motifs\n",
    "\n",
    "    motifLength = int(validation_data[\"length\"].iloc[0])\n",
    "\n",
    "    motifSpots = validation_data[\"start_index\"].tolist()\n",
    "    caseAtSpot = validation_data[\"caseid\"].tolist()\n",
    "    motifAtSpot = validation_data[\"motif\"].tolist()\n",
    "\n",
    "    # Generate Filename\n",
    "    grundTruth_File = log_filename.split(\".\")[0] + \".txt\"\n",
    "    pattern_complete = \"\"\n",
    "    unique_patterns_set = set()\n",
    "    for row in validation_data.itertuples():\n",
    "        pattern = \"\"\n",
    "        for _, row in log.iloc[row[2]:row[2] + row[3]].iterrows():\n",
    "            activityPattern = f\"{row['eventType']}\".strip()\n",
    "            pattern = pattern + activityPattern.replace(\" \",\"\") + \" -1 \"\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Add this line here to clean the final pattern string\n",
    "        pattern = pattern.strip()\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Add only unique patterns\n",
    "        if pattern not in unique_patterns_set:\n",
    "            unique_patterns_set.add(pattern)\n",
    "            pattern_complete = pattern_complete + pattern + \" -2\\n\"\n",
    "        else:\n",
    "            print(\"Duplicate Found, skipping...\")\n",
    "\n",
    "\n",
    "    with open(folder_path + log_filename.split(\".\")[0] + \".txt\", 'w') as f:\n",
    "        f.write(pattern_complete)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddff488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeSeriesData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
