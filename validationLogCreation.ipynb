{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stumpy\n",
    "import numpy as np\n",
    "import time\n",
    "from util.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data based on percentage instead of occurance\n",
    "\n",
    "Set the parameters in the following cell to your desired output. The parameters are described by the comment afterwards.\n",
    "Do not change the parameters after the indication.\n",
    "\n",
    "Execute the following two cells to generate the validation logs based on the SmartRPA validation logs in the folder \"logs/SmartRPA/\".\n",
    "The data will be available after the complete exection. Particularly, the file containing all indexes and parameters will only be available if all loops were executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14498 unique events in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# ---- Setup of the parameters for the logs to be created ----\n",
    "randomness = [1] # Length of sampling sequence, when creating the baseline log (1=> only one event inserted, 2=> sequences of 2 from all possible events inserted ...)\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [5,10,15,20,25,30,60] # Number of motif appearances in the log\n",
    "lengthMotifs = [5,10,15,20,25] # Length of the Motifs to be inserted\n",
    "percentageMotifsOverLog = [10,5,2.5,1,0.01,0.001,0.0005,0.0001] # Percentage representation of the Motif in the log\n",
    "shuffles = [0,10,20] # Percentage by which the inserted routine should be shuffled\n",
    "\n",
    "# ---- Set the Data Path for the csv files used for the data sampling and where the logs should be added ----\n",
    "csvPath = \"logs/smartRPA/\"\n",
    "validation_path = csvPath + \"experiment/\"\n",
    "\n",
    "\n",
    "# DO NOT Change from here for synthetic data!\n",
    "# ---- Columns to generate the validation data for the experiment ----\n",
    "validationDataColumns = [\"uiLogName\",\"variationPercentage\",\"numberOfOccurrancesToBeDiscovered\",\"motifLength\",\"percentageMotifsOverLog\",\"logLength\",\n",
    "                     \"motifSpots\",\"caseIds\"]\n",
    "validationDataDF = pd.DataFrame(columns=validationDataColumns)\n",
    "concept_name_column = 'case:concept:name'\n",
    "timeStampCol = \"time:timestamp\"\n",
    "reduce = 0 # Currently not in use\n",
    "\n",
    "# ---- Gathering of unique events until the upper limit is reached ----\n",
    "dfAll = read_csvs_and_combine(csvPath,1000000)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721          \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df_unique = dfAll.drop_duplicates(subset=subset)\n",
    "print(f\"There are {df_unique.shape[0]} unique events in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI Log of length 250.0 is sampled. Proceeding with insertion.\n",
      "UI Log with name LenLogLong_1_0_1_5_5_10_250.csv completed and stored\n",
      "UI Log with name LenLogLong_1_10_1_5_5_10_250.csv completed and stored\n",
      "UI Log with name LenLogLong_1_20_1_5_5_10_250.csv completed and stored\n",
      "UI Log of length 500.0 is sampled. Proceeding with insertion.\n",
      "UI Log with name LenLogLong_1_0_1_5_10_10_500.csv completed and stored\n",
      "UI Log with name LenLogLong_1_10_1_5_10_10_500.csv completed and stored\n",
      "UI Log with name LenLogLong_1_20_1_5_10_10_500.csv completed and stored\n",
      "UI Log of length 750.0 is sampled. Proceeding with insertion.\n",
      "UI Log with name LenLogLong_1_0_1_5_15_10_750.csv completed and stored\n",
      "UI Log with name LenLogLong_1_10_1_5_15_10_750.csv completed and stored\n",
      "UI Log with name LenLogLong_1_20_1_5_15_10_750.csv completed and stored\n",
      "UI Log of length 1000.0 is sampled. Proceeding with insertion.\n",
      "UI Log with name LenLogLong_1_0_1_5_20_10_1000.csv completed and stored\n",
      "UI Log with name LenLogLong_1_10_1_5_20_10_1000.csv completed and stored\n",
      "UI Log with name LenLogLong_1_20_1_5_20_10_1000.csv completed and stored\n",
      "UI Log of length 1250.0 is sampled. Proceeding with insertion.\n",
      "UI Log with name LenLogLong_1_0_1_5_25_10_1250.csv completed and stored\n",
      "UI Log with name LenLogLong_1_10_1_5_25_10_1250.csv completed and stored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLenLogLong_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshuffle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mocc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmotifLen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(l)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m random_cases_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_random_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfAll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcept_name_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlengthMotifs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m dfAll[dfAll[concept_name_column]\u001b[38;5;241m.\u001b[39misin(random_cases_list)]\n\u001b[0;32m     23\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39miloc[:motifLen]\n",
      "File \u001b[1;32mc:\\Users\\tomho\\OneDrive\\Documents\\VSCode\\TSMDforUILogs\\util\\util.py:286\u001b[0m, in \u001b[0;36mget_random_values\u001b[1;34m(df, column_name, m, min_len)\u001b[0m\n\u001b[0;32m    283\u001b[0m value_counts \u001b[38;5;241m=\u001b[39m df[column_name]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Filter rows based on minimum occurrence\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue_counts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \n\u001b[0;32m    287\u001b[0m random_values \u001b[38;5;241m=\u001b[39m filtered_df[column_name]\u001b[38;5;241m.\u001b[39msample(m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m random_values\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\frame.py:3884\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3882\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3888\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\frame.py:3943\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3940\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m-> 3943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3945\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   3946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\generic.py:6685\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6553\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   6555\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6556\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6557\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6683\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   6684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6685\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6686\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6688\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6689\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\managers.py:587\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    584\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 587\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1750\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2217\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2215\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2217\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2220\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2242\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2235\u001b[0m new_values: ArrayLike\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   2238\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[0;32m   2240\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[1;32m-> 2242\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2244\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---- Loops to create the data and store the file containing the data parameters ----\n",
    "for mot in motifs:\n",
    "    for rand in randomness:\n",
    "        for occ in occurances:\n",
    "            for percentage in percentageMotifsOverLog:\n",
    "                for motifLen in lengthMotifs:\n",
    "                    # Generate a filename and store the file\n",
    "                    \n",
    "                    # Calculate the length of the log based on the percentage values\n",
    "                    l = ((occ*motifLen) / percentage * 100)\n",
    "                    samplingLength = l-(occ*motifLen)\n",
    "\n",
    "                    # Sample the UI log from all available data\n",
    "                    uiLog = get_rand_uiLog(df_unique, n_max=rand, actions=int(samplingLength)) # set to actions=l for proper work\n",
    "                    print(f\"UI Log of length {l} is sampled. Proceeding with insertion.\")\n",
    "                    for shuffle in shuffles:\n",
    "                        if percentage == 2.5:\n",
    "                            filename = f\"LenLog_{rand}_{shuffle}_{mot}_{occ}_{motifLen}_2-5_{int(l)}.csv\"\n",
    "                        else:\n",
    "                            filename = f\"LenLog_{rand}_{shuffle}_{mot}_{occ}_{motifLen}_{percentage}_{int(l)}.csv\"\n",
    "                        random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "                        filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "                        filtered_df = filtered_df.iloc[:motifLen]\n",
    "                        \n",
    "                        uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                            uiLog=uiLog,\n",
    "                                                            dfcases=filtered_df,\n",
    "                                                            occurances=occ,\n",
    "                                                            case_column_name=concept_name_column,\n",
    "                                                            sorted_insert_col=timeStampCol,\n",
    "                                                            shuffled=True,\n",
    "                                                            shuffled_by=shuffle+1,\n",
    "                                                            reduced=False,\n",
    "                                                            reduced_by=reduce)\n",
    "                        \n",
    "                        new_row = {'uiLogName': filename, \"variationPercentage\":  shuffle, \"motifsToBeDiscovered\": mot, \"numberOfOccurrancesToBeDiscovered\": occ,\n",
    "                        \"motifLength\": motifLen, \"percentageMotifsOverLog\": percentage, \"logLength\": l, \"motifSpots\": indices, \"caseIds\": random_cases_list}\n",
    "                        filepath = validation_path + filename\n",
    "                        uiLog.to_csv(filepath, index=False)\n",
    "                        print(f\"UI Log with name {filename} completed and stored\")\n",
    "                        # It is know that there is a FutureWarning for all-na or empty rows. Based on the previous logic everythin will work.\n",
    "                        validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n",
    "\n",
    "validationDataDF.to_csv(validation_path + \"long_logs_validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive - Not in Use\n",
    "\n",
    "### Creation of artificial validation logs for TSDM Discovery in UI logs\n",
    "\n",
    "Method:\n",
    "  1. Get user interactions (a) and create a set of user actions (A)\n",
    "  2. Select random actions (1 to n consequtive actions per looping) append them into a dataframe (D) until a upper limit (x) is reached\n",
    "        - The upper limit x is considered as 1 action per 3 seconds in a 8 hour work day => 8* 60 * (60/3) = 9600 actions a day\n",
    "  4. Get routines (r) (1-m overall) and insert the routines (r) o-times at random points into the dataframe (D)\n",
    "        - The routines need not interrupt themselfs, otherwise no motif could be discovered (for future tests, the could interrupt as well)\n",
    "\n",
    "Result: A dataframe (D) with x + (o * len(r)) number of actions containing m routines at random points\n",
    "\n",
    "We create a set of UI logs which vary in the following constraints:\n",
    "- Randomness of actions inserted into the logs\n",
    "    - The actions can be sampled completly randomly\n",
    "    - The actions can be consequitive actions taken from previous logs\n",
    "- Number of motifs inserted\n",
    "    - The length of the motif is defined by the original motif in the smartRPA log\n",
    "- Number of times the motifs are inserted\n",
    "- Length of the UI log\n",
    "\n",
    "Objective: Create a dataframe that mimics a long time recording of users, which contains routines\n",
    "\n",
    "The file name will be created as follows\n",
    "\"Log_ [#Randomness]_ [#Motifs] _[#MotifOccurance] _[#LogLength]_[#ShuffledBy]_[#ReducedBy].csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Set the Data Path for the csv files used for the data sampling and where the logs should be added ----\n",
    "csvPath = \"logs/smartRPA/\"\n",
    "validation_path = csvPath + \"validation/\"\n",
    "\n",
    "randomness = [1] # how long should the original action sequences be\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10,15,20,30,60] #motifs count of the motif occurances in the log\n",
    "lengthLog = [1000,2000,4000,8000,12000,15000,17500,20000,25000,30000] # 9600 are Events for approximatly one working day\n",
    "percentageMotifsOverLog = [1,2.5,5,10]\n",
    "\n",
    "# ToDo or just take the original size of the case to make it more real world\n",
    "lengthMotifs = [5,10,15,20,25] # Could be added to enter different length motifs into the data\n",
    "\n",
    "# Shuffle and reduction of the event log\n",
    "shuffle = 15\n",
    "reduce = 15\n",
    "\n",
    "concept_name_column = 'case:concept:name'\n",
    "timeStampCol = \"time:timestamp\"\n",
    "\n",
    "dfAll = read_csvs_and_combine(csvPath,1000000)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721          \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df_unique = dfAll.drop_duplicates(subset=subset)\n",
    "print(f\"There are {df_unique.shape[0]} unique events in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': []}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    # Not used at the moment as we have changed to percentage based calculation\n",
    "    for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "        beginningLog = get_rand_uiLog(dfAll, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "        for occ in occurances:\n",
    "            for mot in motifs: # Number of motifs\n",
    "                uiLog = beginningLog\n",
    "                random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=15)\n",
    "                # Filter rows with values in the list, because the length is shorter for the following loop\n",
    "                filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "                uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                           uiLog=uiLog,\n",
    "                                                           dfcases=filtered_df,\n",
    "                                                           occurances=occ,\n",
    "                                                           case_column_name=concept_name_column,\n",
    "                                                           sorted_insert_col=timeStampCol,\n",
    "                                                           shuffled=False,\n",
    "                                                           shuffled_by=shuffle,\n",
    "                                                           reduced=False,\n",
    "                                                           reduced_by=reduce)\n",
    "                filename = f\"Log_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}_.csv\"\n",
    "                filepath = validation_path + filename\n",
    "                print(filename)\n",
    "                uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "                # For tracking purpose and validation: Store the index of the cases in each log\n",
    "                row = (filename, str(indices), str(random_cases_list)) \n",
    "                new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "                index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"validation_data_high_percentage.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously part created motifs have all the same length. However, the data in our approach can handle different length motifs as well.\n",
    "We use the work in doi.org/10.1109/ACCESS.2023.3295995 to identif such motifs.\n",
    "The logs created here contain different length sized motifs, resembling different length user tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': [], 'CaseLength':[]}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    for occ in occurances:\n",
    "        for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "            uiLog = get_rand_uiLog(df_unique, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "            mot = len(lengthMotifs)\n",
    "            random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "            filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "            # Reduce the cases in length and append again\n",
    "            strCaseLength = \"\"\n",
    "            for i, element in enumerate(lengthMotifs):\n",
    "                insert_df = filtered_df[filtered_df[concept_name_column] == random_cases_list[i]].sort_values(timeStampCol)\n",
    "                try:\n",
    "                    variableLengthDf = pd.concat([insert_df.iloc[:element], variableLengthDf], ignore_index=True)\n",
    "                except NameError:\n",
    "                    variableLengthDf = insert_df.iloc[:element-1]\n",
    "                strCaseLength = strCaseLength + f\"{random_cases_list[i]}:{element}/\"\n",
    "\n",
    "            uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                        uiLog=uiLog,\n",
    "                                                        dfcases=variableLengthDf,\n",
    "                                                        occurances=occ,\n",
    "                                                        case_column_name=concept_name_column,\n",
    "                                                        sorted_insert_col=timeStampCol,\n",
    "                                                        shuffled=True,\n",
    "                                                        shuffled_by=shuffle,\n",
    "                                                        reduced=True,\n",
    "                                                        reduced_by=reduce)\n",
    "        \n",
    "            filename = f\"VarLenLog_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}.csv\"\n",
    "            filepath = validation_path + filename\n",
    "            uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "            # For tracking purpose and validation: Store the index of the cases in each log\n",
    "            row = (filename, str(indices), str(random_cases_list), strCaseLength) \n",
    "            new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "            index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"var_len_validation_data.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeSeriesData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
