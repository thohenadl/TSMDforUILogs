{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stumpy\n",
    "import numpy as np\n",
    "import time\n",
    "from util.util import *\n",
    "csvPath = \"logs/smartRPA/\"\n",
    "validation_path = csvPath + \"validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of artificial validation logs for TSDM Discovery in UI logs\n",
    "\n",
    "Method:\n",
    "  1. Get user interactions (a) and create a set of user actions (A)\n",
    "  2. Select random actions (1 to n consequtive actions per looping) append them into a dataframe (D) until a upper limit (x) is reached\n",
    "        - The upper limit x is considered as 1 action per 3 seconds in a 8 hour work day => 8* 60 * (60/3) = 9600 actions a day\n",
    "  4. Get routines (r) (1-m overall) and insert the routines (r) o-times at random points into the dataframe (D)\n",
    "        - The routines need not interrupt themselfs, otherwise no motif could be discovered (for future tests, the could interrupt as well)\n",
    "\n",
    "Result: A dataframe (D) with x + (o * len(r)) number of actions containing m routines at random points\n",
    "\n",
    "We create a set of UI logs which vary in the following constraints:\n",
    "- Randomness of actions inserted into the logs\n",
    "    - The actions can be sampled completly randomly\n",
    "    - The actions can be consequitive actions taken from previous logs\n",
    "- Number of motifs inserted\n",
    "    - The length of the motif is defined by the original motif in the smartRPA log\n",
    "- Number of times the motifs are inserted\n",
    "- Length of the UI log\n",
    "\n",
    "Objective: Create a dataframe that mimics a long time recording of users, which contains routines\n",
    "\n",
    "\n",
    "\n",
    "The file name will be created as follows\n",
    "\"Log_ [#Randomness]_ [#Motifs] _[#MotifOccurance] _[#LogLength]_[#ShuffledBy]_[#ReducedBy].csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14498 unique events in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "randomness = [1] # how long should the original action sequences be\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10,15,20,30,60] #motifs count of the motif occurances in the log\n",
    "lengthLog = [1000,2000,4000,8000,12000,15000,17500,20000,25000,30000] # 9600 are Events for approximatly one working day\n",
    "percentageMotifsOverLog = [1,2.5,5,10]\n",
    "\n",
    "# ToDo or just take the original size of the case to make it more real world\n",
    "lengthMotifs = [5,10,15,20,25] # Could be added to enter different length motifs into the data\n",
    "\n",
    "# Shuffle and reduction of the event log\n",
    "shuffle = 15\n",
    "reduce = 15\n",
    "\n",
    "concept_name_column = 'case:concept:name'\n",
    "timeStampCol = \"time:timestamp\"\n",
    "\n",
    "dfAll = read_csvs_and_combine(csvPath,1000000)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721          \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df_unique = dfAll.drop_duplicates(subset=subset)\n",
    "print(f\"There are {df_unique.shape[0]} unique events in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': []}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    # Not used at the moment as we have changed to percentage based calculation\n",
    "    for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "        beginningLog = get_rand_uiLog(dfAll, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "        for occ in occurances:\n",
    "            for mot in motifs: # Number of motifs\n",
    "                uiLog = beginningLog\n",
    "                random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=15)\n",
    "                # Filter rows with values in the list, because the length is shorter for the following loop\n",
    "                filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "                uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                           uiLog=uiLog,\n",
    "                                                           dfcases=filtered_df,\n",
    "                                                           occurances=occ,\n",
    "                                                           case_column_name=concept_name_column,\n",
    "                                                           sorted_insert_col=timeStampCol,\n",
    "                                                           shuffled=False,\n",
    "                                                           shuffled_by=shuffle,\n",
    "                                                           reduced=False,\n",
    "                                                           reduced_by=reduce)\n",
    "                filename = f\"Log_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}_.csv\"\n",
    "                filepath = validation_path + filename\n",
    "                print(filename)\n",
    "                uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "                # For tracking purpose and validation: Store the index of the cases in each log\n",
    "                row = (filename, str(indices), str(random_cases_list)) \n",
    "                new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "                index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"validation_data_high_percentage.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously part created motifs have all the same length. However, the data in our approach can handle different length motifs as well.\n",
    "We use the work in doi.org/10.1109/ACCESS.2023.3295995 to identif such motifs.\n",
    "The logs created here contain different length sized motifs, resembling different length user tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': [], 'CaseLength':[]}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    for occ in occurances:\n",
    "        for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "            uiLog = get_rand_uiLog(df_unique, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "            mot = len(lengthMotifs)\n",
    "            random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "            filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "            # Reduce the cases in length and append again\n",
    "            strCaseLength = \"\"\n",
    "            for i, element in enumerate(lengthMotifs):\n",
    "                insert_df = filtered_df[filtered_df[concept_name_column] == random_cases_list[i]].sort_values(timeStampCol)\n",
    "                try:\n",
    "                    variableLengthDf = pd.concat([insert_df.iloc[:element], variableLengthDf], ignore_index=True)\n",
    "                except NameError:\n",
    "                    variableLengthDf = insert_df.iloc[:element-1]\n",
    "                strCaseLength = strCaseLength + f\"{random_cases_list[i]}:{element}/\"\n",
    "\n",
    "            uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                        uiLog=uiLog,\n",
    "                                                        dfcases=variableLengthDf,\n",
    "                                                        occurances=occ,\n",
    "                                                        case_column_name=concept_name_column,\n",
    "                                                        sorted_insert_col=timeStampCol,\n",
    "                                                        shuffled=True,\n",
    "                                                        shuffled_by=shuffle,\n",
    "                                                        reduced=True,\n",
    "                                                        reduced_by=reduce)\n",
    "        \n",
    "            filename = f\"VarLenLog_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}.csv\"\n",
    "            filepath = validation_path + filename\n",
    "            uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "            # For tracking purpose and validation: Store the index of the cases in each log\n",
    "            row = (filename, str(indices), str(random_cases_list), strCaseLength) \n",
    "            new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "            index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"var_len_validation_data.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data based on percentage instead of occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_11788\\2862136750.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.31 GiB for an array with shape (9, 19509088) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m dfAll[dfAll[concept_name_column]\u001b[38;5;241m.\u001b[39misin(random_cases_list)]\n\u001b[0;32m     34\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39miloc[:motifLen]\n\u001b[1;32m---> 36\u001b[0m uiLog, indices, random_cases_list \u001b[38;5;241m=\u001b[39m \u001b[43minsert_motifs_non_overlap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_cases_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_cases_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43muiLog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muiLog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdfcases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiltered_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moccurances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcase_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcept_name_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msorted_insert_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeStampCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mshuffled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mshuffled_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mreduced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mreduced_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m new_row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muiLogName\u001b[39m\u001b[38;5;124m'\u001b[39m: filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariationPercentage\u001b[39m\u001b[38;5;124m\"\u001b[39m:  shuffle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotifsToBeDiscovered\u001b[39m\u001b[38;5;124m\"\u001b[39m: mot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumberOfOccurrancesToBeDiscovered\u001b[39m\u001b[38;5;124m\"\u001b[39m: occ,\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotifLength\u001b[39m\u001b[38;5;124m\"\u001b[39m: motifLen, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpercentageMotifsOverLog\u001b[39m\u001b[38;5;124m\"\u001b[39m: percentage, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogLength\u001b[39m\u001b[38;5;124m\"\u001b[39m: l, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotifSpots\u001b[39m\u001b[38;5;124m\"\u001b[39m: indices, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaseIds\u001b[39m\u001b[38;5;124m\"\u001b[39m: random_cases_list}\n\u001b[0;32m     49\u001b[0m filepath \u001b[38;5;241m=\u001b[39m validation_path \u001b[38;5;241m+\u001b[39m filename\n",
      "File \u001b[1;32mc:\\Users\\tomho\\OneDrive\\Documents\\VSCode\\TSMDforUILogs\\util\\util.py:413\u001b[0m, in \u001b[0;36minsert_motifs_non_overlap\u001b[1;34m(random_cases_list, uiLog, dfcases, occurances, case_column_name, sorted_insert_col, shuffled, shuffled_by, reduced, reduced_by)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffled:\n\u001b[0;32m    411\u001b[0m     insert_df \u001b[38;5;241m=\u001b[39m reorder_dataframe(insert_df,shuffled_by)\n\u001b[1;32m--> 413\u001b[0m uiLog \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43muiLog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minsert_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muiLog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Correct the indexes by the length of the inserted dataframe (routine)\u001b[39;00m\n\u001b[0;32m    415\u001b[0m index_list \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(insert_df) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_list[:i]] \u001b[38;5;241m+\u001b[39m index_list[i:]\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    678\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 680\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    684\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\concat.py:466\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    463\u001b[0m has_none_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(unit\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m join_units)\n\u001b[0;32m    464\u001b[0m upcasted_na \u001b[38;5;241m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[1;32m--> 466\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    467\u001b[0m     ju\u001b[38;5;241m.\u001b[39mget_reindexed_values(empty_dtype\u001b[38;5;241m=\u001b[39mempty_dtype, upcasted_na\u001b[38;5;241m=\u001b[39mupcasted_na)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units\n\u001b[0;32m    469\u001b[0m ]\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat):\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special case not needed if all EAs used HybridBlocks\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# argument type \"Tuple[int, slice]\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m         t\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m t[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat\n\u001b[0;32m    481\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\concat.py:467\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    463\u001b[0m has_none_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(unit\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m join_units)\n\u001b[0;32m    464\u001b[0m upcasted_na \u001b[38;5;241m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[0;32m    466\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 467\u001b[0m     \u001b[43mju\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reindexed_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupcasted_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcasted_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units\n\u001b[0;32m    469\u001b[0m ]\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat):\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special case not needed if all EAs used HybridBlocks\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# argument type \"Tuple[int, slice]\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m         t\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m t[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat\n\u001b[0;32m    481\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\concat.py:452\u001b[0m, in \u001b[0;36mJoinUnit.get_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mand\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m             fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_na_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\tomho\\anaconda3\\envs\\timeSeriesData\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2316\u001b[0m, in \u001b[0;36mmake_na_array\u001b[1;34m(dtype, shape, fill_value)\u001b[0m\n\u001b[0;32m   2312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m missing_arr\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2314\u001b[0m     \u001b[38;5;66;03m# NB: we should never get here with dtype integer or bool;\u001b[39;00m\n\u001b[0;32m   2315\u001b[0m     \u001b[38;5;66;03m#  if we did, the missing_arr.fill would cast to gibberish\u001b[39;00m\n\u001b[1;32m-> 2316\u001b[0m     missing_arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2317\u001b[0m     missing_arr\u001b[38;5;241m.\u001b[39mfill(fill_value)\n\u001b[0;32m   2319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.31 GiB for an array with shape (9, 19509088) and data type float64"
     ]
    }
   ],
   "source": [
    "validationDataColumns = [\"uiLogName\",\"variationPercentage\",\"numberOfOccurrancesToBeDiscovered\",\"motifLength\",\"percentageMotifsOverLog\",\"logLength\",\n",
    "                     \"motifSpots\",\"caseIds\"]\n",
    "validationDataDF = pd.DataFrame(columns=validationDataColumns)\n",
    "validation_path = csvPath + \"percentageComparison/\"\n",
    "\n",
    "randomness = [1] # how long should the original action sequences be\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10] #motifs count of the motif occurances in the log\n",
    "lengthMotifs = [10] # Length of the Motifs to be inserted\n",
    "percentageMotifsOverLog = [0.01,0.001,0.0005,0.0001]\n",
    "shuffles = [0]\n",
    "\n",
    "\n",
    "for mot in motifs:\n",
    "    for rand in randomness:\n",
    "        for occ in occurances:\n",
    "            for percentage in percentageMotifsOverLog:\n",
    "                for motifLen in lengthMotifs:\n",
    "                    # Generate a filename and store the file\n",
    "                    \n",
    "                    # Calculate the length of the log based on the percentage values\n",
    "                    l = ((occ*motifLen) / percentage * 100)\n",
    "                    samplingLength = l-(occ*motifLen)\n",
    "\n",
    "                    # Sample the UI log from all available data\n",
    "                    uiLog = get_rand_uiLog(df_unique, n_max=rand, actions=int(samplingLength)) # set to actions=l for proper work\n",
    "                    for shuffle in shuffles:\n",
    "                        if percentage == 2.5:\n",
    "                            filename = f\"LenLogLong_{rand}_{shuffle}_{mot}_{occ}_{motifLen}_2-5_{int(l)}.csv\"\n",
    "                        else:\n",
    "                            filename = f\"LenLogLong_{rand}_{shuffle}_{mot}_{occ}_{motifLen}_{percentage}_{int(l)}.csv\"\n",
    "                        random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "                        filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "                        filtered_df = filtered_df.iloc[:motifLen]\n",
    "                        \n",
    "                        uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                            uiLog=uiLog,\n",
    "                                                            dfcases=filtered_df,\n",
    "                                                            occurances=occ,\n",
    "                                                            case_column_name=concept_name_column,\n",
    "                                                            sorted_insert_col=timeStampCol,\n",
    "                                                            shuffled=True,\n",
    "                                                            shuffled_by=shuffle+1,\n",
    "                                                            reduced=False,\n",
    "                                                            reduced_by=reduce)\n",
    "                        \n",
    "                        new_row = {'uiLogName': filename, \"variationPercentage\":  shuffle, \"motifsToBeDiscovered\": mot, \"numberOfOccurrancesToBeDiscovered\": occ,\n",
    "                        \"motifLength\": motifLen, \"percentageMotifsOverLog\": percentage, \"logLength\": l, \"motifSpots\": indices, \"caseIds\": random_cases_list}\n",
    "                        filepath = validation_path + filename\n",
    "                        uiLog.to_csv(filepath, index=False)\n",
    "                        validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n",
    "\n",
    "validationDataDF.to_csv(validation_path + \"long_logs_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationDataDF.to_csv(validation_path + \"long_logs_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeSeriesData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
