{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stumpy\n",
    "import numpy as np\n",
    "import time\n",
    "from util.util import *\n",
    "csvPath = \"logs/smartRPA/\"\n",
    "validation_path = csvPath + \"validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of artificial validation logs for TSDM Discovery in UI logs\n",
    "\n",
    "Method:\n",
    "  1. Get user interactions (a) and create a set of user actions (A)\n",
    "  2. Select random actions (1 to n consequtive actions per looping) append them into a dataframe (D) until a upper limit (x) is reached\n",
    "        - The upper limit x is considered as 1 action per 3 seconds in a 8 hour work day => 8* 60 * (60/3) = 9600 actions a day\n",
    "  4. Get routines (r) (1-m overall) and insert the routines (r) o-times at random points into the dataframe (D)\n",
    "        - The routines need not interrupt themselfs, otherwise no motif could be discovered (for future tests, the could interrupt as well)\n",
    "\n",
    "Result: A dataframe (D) with x + (o * len(r)) number of actions containing m routines at random points\n",
    "\n",
    "We create a set of UI logs which vary in the following constraints:\n",
    "- Randomness of actions inserted into the logs\n",
    "    - The actions can be sampled completly randomly\n",
    "    - The actions can be consequitive actions taken from previous logs\n",
    "- Number of motifs inserted\n",
    "    - The length of the motif is defined by the original motif in the smartRPA log\n",
    "- Number of times the motifs are inserted\n",
    "- Length of the UI log\n",
    "\n",
    "Objective: Create a dataframe that mimics a long time recording of users, which contains routines\n",
    "\n",
    "\n",
    "\n",
    "The file name will be created as follows\n",
    "\"Log_ [#Randomness]_ [#Motifs] _[#MotifOccurance] _[#LogLength]_[#ShuffledBy]_[#ReducedBy].csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14498 unique events in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "randomness = [1] # how long should the original action sequences be\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10,15,20,30,60] #motifs count of the motif occurances in the log\n",
    "lengthLog = [1000,2000,4000,8000,12000,15000,17500,20000,25000,30000] # 9600 are Events for approximatly one working day\n",
    "percentageMotifsOverLog = [1,2.5,5,10]\n",
    "\n",
    "# ToDo or just take the original size of the case to make it more real world\n",
    "lengthMotifs = [5,10,15,20,25] # Could be added to enter different length motifs into the data\n",
    "\n",
    "# Shuffle and reduction of the event log\n",
    "shuffle = 0\n",
    "reduce = 0\n",
    "\n",
    "concept_name_column = 'case:concept:name'\n",
    "timeStampCol = \"time:timestamp\"\n",
    "\n",
    "dfAll = read_csvs_and_combine(csvPath,1000000)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721          \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df_unique = dfAll.drop_duplicates(subset=subset)\n",
    "print(f\"There are {df_unique.shape[0]} unique events in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': []}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    # Not used at the moment as we have changed to percentage based calculation\n",
    "    for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "        beginningLog = get_rand_uiLog(dfAll, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "        for occ in occurances:\n",
    "            for mot in motifs: # Number of motifs\n",
    "                uiLog = beginningLog\n",
    "                random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=15)\n",
    "                # Filter rows with values in the list, because the length is shorter for the following loop\n",
    "                filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "                uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                           uiLog=uiLog,\n",
    "                                                           dfcases=filtered_df,\n",
    "                                                           occurances=occ,\n",
    "                                                           case_column_name=concept_name_column,\n",
    "                                                           sorted_insert_col=timeStampCol,\n",
    "                                                           shuffled=False,\n",
    "                                                           shuffled_by=shuffle,\n",
    "                                                           reduced=False,\n",
    "                                                           reduced_by=reduce)\n",
    "                filename = f\"Log_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}_.csv\"\n",
    "                filepath = validation_path + filename\n",
    "                print(filename)\n",
    "                uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "                # For tracking purpose and validation: Store the index of the cases in each log\n",
    "                row = (filename, str(indices), str(random_cases_list)) \n",
    "                new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "                index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"validation_data_high_percentage.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously part created motifs have all the same length. However, the data in our approach can handle different length motifs as well.\n",
    "We use the work in doi.org/10.1109/ACCESS.2023.3295995 to identif such motifs.\n",
    "The logs created here contain different length sized motifs, resembling different length user tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': [], 'CaseLength':[]}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    for occ in occurances:\n",
    "        for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "            uiLog = get_rand_uiLog(dfAll, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "            mot = len(lengthMotifs)\n",
    "            random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "            filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "            # Reduce the cases in length and append again\n",
    "            strCaseLength = \"\"\n",
    "            for i, element in enumerate(lengthMotifs):\n",
    "                insert_df = filtered_df[filtered_df[concept_name_column] == random_cases_list[i]].sort_values(timeStampCol)\n",
    "                try:\n",
    "                    variableLengthDf = pd.concat([insert_df.iloc[:element], variableLengthDf], ignore_index=True)\n",
    "                except NameError:\n",
    "                    variableLengthDf = insert_df.iloc[:element-1]\n",
    "                strCaseLength = strCaseLength + f\"{random_cases_list[i]}:{element}/\"\n",
    "\n",
    "            uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                        uiLog=uiLog,\n",
    "                                                        dfcases=variableLengthDf,\n",
    "                                                        occurances=occ,\n",
    "                                                        case_column_name=concept_name_column,\n",
    "                                                        sorted_insert_col=timeStampCol,\n",
    "                                                        shuffled=True,\n",
    "                                                        shuffled_by=shuffle,\n",
    "                                                        reduced=True,\n",
    "                                                        reduced_by=reduce)\n",
    "        \n",
    "            filename = f\"VarLenLog_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}.csv\"\n",
    "            filepath = validation_path + filename\n",
    "            uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "            # For tracking purpose and validation: Store the index of the cases in each log\n",
    "            row = (filename, str(indices), str(random_cases_list), strCaseLength) \n",
    "            new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "            index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"var_len_validation_data.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data based on percentage instead of occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LenLog_1_1_10_5_1_5000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_10032\\1134609141.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LenLog_1_1_10_10_1_10000.csv\n",
      "LenLog_1_1_10_15_1_15000.csv\n",
      "LenLog_1_1_10_20_1_20000.csv\n",
      "LenLog_1_1_10_25_1_25000.csv\n",
      "LenLog_1_1_10_5_2-5_2000.csv\n",
      "LenLog_1_1_10_10_2-5_4000.csv\n",
      "LenLog_1_1_10_15_2-5_6000.csv\n",
      "LenLog_1_1_10_20_2-5_8000.csv\n",
      "LenLog_1_1_10_25_2-5_10000.csv\n",
      "LenLog_1_1_10_5_5_1000.csv\n",
      "LenLog_1_1_10_10_5_2000.csv\n",
      "LenLog_1_1_10_15_5_3000.csv\n",
      "LenLog_1_1_10_20_5_4000.csv\n",
      "LenLog_1_1_10_25_5_5000.csv\n",
      "LenLog_1_1_10_5_10_500.csv\n",
      "LenLog_1_1_10_10_10_1000.csv\n",
      "LenLog_1_1_10_15_10_1500.csv\n",
      "LenLog_1_1_10_20_10_2000.csv\n",
      "LenLog_1_1_10_25_10_2500.csv\n",
      "LenLog_1_1_15_5_1_7500.csv\n",
      "LenLog_1_1_15_10_1_15000.csv\n",
      "LenLog_1_1_15_15_1_22500.csv\n",
      "LenLog_1_1_15_20_1_30000.csv\n",
      "LenLog_1_1_15_25_1_37500.csv\n",
      "LenLog_1_1_15_5_2-5_3000.csv\n",
      "LenLog_1_1_15_10_2-5_6000.csv\n",
      "LenLog_1_1_15_15_2-5_9000.csv\n",
      "LenLog_1_1_15_20_2-5_12000.csv\n",
      "LenLog_1_1_15_25_2-5_15000.csv\n",
      "LenLog_1_1_15_5_5_1500.csv\n",
      "LenLog_1_1_15_10_5_3000.csv\n",
      "LenLog_1_1_15_15_5_4500.csv\n",
      "LenLog_1_1_15_20_5_6000.csv\n",
      "LenLog_1_1_15_25_5_7500.csv\n",
      "LenLog_1_1_15_5_10_750.csv\n",
      "LenLog_1_1_15_10_10_1500.csv\n",
      "LenLog_1_1_15_15_10_2250.csv\n",
      "LenLog_1_1_15_20_10_3000.csv\n",
      "LenLog_1_1_15_25_10_3750.csv\n",
      "LenLog_1_1_20_5_1_10000.csv\n",
      "LenLog_1_1_20_10_1_20000.csv\n",
      "LenLog_1_1_20_15_1_30000.csv\n",
      "LenLog_1_1_20_20_1_40000.csv\n",
      "LenLog_1_1_20_25_1_50000.csv\n",
      "LenLog_1_1_20_5_2-5_4000.csv\n",
      "LenLog_1_1_20_10_2-5_8000.csv\n",
      "LenLog_1_1_20_15_2-5_12000.csv\n",
      "LenLog_1_1_20_20_2-5_16000.csv\n",
      "LenLog_1_1_20_25_2-5_20000.csv\n",
      "LenLog_1_1_20_5_5_2000.csv\n",
      "LenLog_1_1_20_10_5_4000.csv\n",
      "LenLog_1_1_20_15_5_6000.csv\n",
      "LenLog_1_1_20_20_5_8000.csv\n",
      "LenLog_1_1_20_25_5_10000.csv\n",
      "LenLog_1_1_20_5_10_1000.csv\n",
      "LenLog_1_1_20_10_10_2000.csv\n",
      "LenLog_1_1_20_15_10_3000.csv\n",
      "LenLog_1_1_20_20_10_4000.csv\n",
      "LenLog_1_1_20_25_10_5000.csv\n",
      "LenLog_1_1_30_5_1_15000.csv\n",
      "LenLog_1_1_30_10_1_30000.csv\n",
      "LenLog_1_1_30_15_1_45000.csv\n",
      "LenLog_1_1_30_20_1_60000.csv\n",
      "LenLog_1_1_30_25_1_75000.csv\n",
      "LenLog_1_1_30_5_2-5_6000.csv\n",
      "LenLog_1_1_30_10_2-5_12000.csv\n",
      "LenLog_1_1_30_15_2-5_18000.csv\n",
      "LenLog_1_1_30_20_2-5_24000.csv\n",
      "LenLog_1_1_30_25_2-5_30000.csv\n",
      "LenLog_1_1_30_5_5_3000.csv\n",
      "LenLog_1_1_30_10_5_6000.csv\n",
      "LenLog_1_1_30_15_5_9000.csv\n",
      "LenLog_1_1_30_20_5_12000.csv\n",
      "LenLog_1_1_30_25_5_15000.csv\n",
      "LenLog_1_1_30_5_10_1500.csv\n",
      "LenLog_1_1_30_10_10_3000.csv\n",
      "LenLog_1_1_30_15_10_4500.csv\n",
      "LenLog_1_1_30_20_10_6000.csv\n",
      "LenLog_1_1_30_25_10_7500.csv\n",
      "LenLog_1_1_60_5_1_30000.csv\n",
      "LenLog_1_1_60_10_1_60000.csv\n",
      "LenLog_1_1_60_15_1_90000.csv\n",
      "LenLog_1_1_60_20_1_120000.csv\n",
      "LenLog_1_1_60_25_1_150000.csv\n",
      "LenLog_1_1_60_5_2-5_12000.csv\n",
      "LenLog_1_1_60_10_2-5_24000.csv\n",
      "LenLog_1_1_60_15_2-5_36000.csv\n",
      "LenLog_1_1_60_20_2-5_48000.csv\n",
      "LenLog_1_1_60_25_2-5_60000.csv\n",
      "LenLog_1_1_60_5_5_6000.csv\n",
      "LenLog_1_1_60_10_5_12000.csv\n",
      "LenLog_1_1_60_15_5_18000.csv\n",
      "LenLog_1_1_60_20_5_24000.csv\n",
      "LenLog_1_1_60_25_5_30000.csv\n",
      "LenLog_1_1_60_5_10_3000.csv\n",
      "LenLog_1_1_60_10_10_6000.csv\n",
      "LenLog_1_1_60_15_10_9000.csv\n",
      "LenLog_1_1_60_20_10_12000.csv\n",
      "LenLog_1_1_60_25_10_15000.csv\n"
     ]
    }
   ],
   "source": [
    "validationDataColumns = [\"uiLogName\",\"variationPercentage\",\"numberOfOccurrancesToBeDiscovered\",\"motifLength\",\"percentageMotifsOverLog\",\"logLength\",\n",
    "                     \"motifSpots\",\"caseIds\"]\n",
    "validationDataDF = pd.DataFrame(columns=validationDataColumns)\n",
    "validation_path = csvPath + \"percentageComparison/\"\n",
    "\n",
    "randomness = [1] # how long should the original action sequences be\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10,15,20,30,60] #motifs count of the motif occurances in the log\n",
    "percentageMotifsOverLog = [1,2.5,5,10]\n",
    "\n",
    "# ToDo or just take the original size of the case to make it more real world\n",
    "lengthMotifs = [5,10,15,20,25] # Could be added to enter different length motifs into the data\n",
    "\n",
    "\n",
    "for mot in motifs:\n",
    "    for rand in randomness:\n",
    "        for occ in occurances:\n",
    "            for percentage in percentageMotifsOverLog:\n",
    "                for motifLen in lengthMotifs:\n",
    "                    # Generate a filename and store the file\n",
    "                    \n",
    "                    # Calculate the length of the log based on the percentage values\n",
    "                    l = ((occ*motifLen)/percentage*100)\n",
    "                    samplingLength = l-(occ*motifLen)\n",
    "\n",
    "                    if percentage == 2.5:\n",
    "                        filename = f\"LenLog_{rand}_{mot}_{occ}_{motifLen}_2-5_{int(l)}.csv\"\n",
    "                    else:\n",
    "                        filename = f\"LenLog_{rand}_{mot}_{occ}_{motifLen}_{percentage}_{int(l)}.csv\"\n",
    "                    \n",
    "                    # Sample the UI log from all available data\n",
    "                    uiLog = get_rand_uiLog(dfAll, n_max=rand, actions=int(samplingLength)) # set to actions=l for proper work\n",
    "                    random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "                    filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "                    filtered_df = filtered_df.iloc[:motifLen]\n",
    "                    \n",
    "                    uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                        uiLog=uiLog,\n",
    "                                                        dfcases=filtered_df,\n",
    "                                                        occurances=occ,\n",
    "                                                        case_column_name=concept_name_column,\n",
    "                                                        sorted_insert_col=timeStampCol,\n",
    "                                                        shuffled=True,\n",
    "                                                        shuffled_by=shuffle,\n",
    "                                                        reduced=True,\n",
    "                                                        reduced_by=reduce)\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    new_row = {'uiLogName': filename, \"variationPercentage\":  shuffle, \"motifsToBeDiscovered\": mot, \"numberOfOccurrancesToBeDiscovered\": occ,\n",
    "                    \"motifLength\": motifLen, \"percentageMotifsOverLog\": percentage, \"logLength\": l, \"motifSpots\": indices, \"caseIds\": random_cases_list}\n",
    "                    print(filename)\n",
    "                    filepath = validation_path + filename\n",
    "                    uiLog.to_csv(filepath, index=False)\n",
    "                    \n",
    "                    validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n",
    "\n",
    "\n",
    "validationDataDF.to_csv(validation_path + \"validationDataPercentage.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeSeriesData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
