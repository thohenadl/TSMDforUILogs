{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stumpy\n",
    "import numpy as np\n",
    "import time\n",
    "from util.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data based on percentage instead of occurance\n",
    "\n",
    "Set the parameters in the following cell to your desired output. The parameters are described by the comment afterwards.\n",
    "Do not change the parameters after the indication.\n",
    "\n",
    "Execute the following two cells to generate the validation logs based on the SmartRPA validation logs in the folder \"logs/SmartRPA/\".\n",
    "The data will be available after the complete exection. Particularly, the file containing all indexes and parameters will only be available if all loops were executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14498 unique events in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# ---- Setup of the parameters for the logs to be created ----\n",
    "randomness = [1] # Length of sampling sequence, when creating the baseline log (1=> only one event inserted, 2=> sequences of 2 from all possible events inserted ...)\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10] # Number of motif appearances in the log >> r_o in the paper\n",
    "# !!!! occurances should be large enough, if motifs >1 to 1. fit all motifs, and 2. make it possible that at random choice the motifs are added as well. There is currently no fix\n",
    "lengthMotifs = [25] # Length of the Motifs to be inserted >> r_len in the paper\n",
    "percentageMotifsOverLog = [100] # Percentage representation of the Motif in the log >> p in the paper\n",
    "shuffles = [0] # Percentage by which the inserted routine should be shuffled >> v_in in the paper\n",
    "\n",
    "# ---- Set the Data Path for the csv files used for the data sampling and where the logs should be added ----\n",
    "csvPath = \"logs/smartRPA/OriginalAgostinelliLogs/\"\n",
    "validation_path = \"logs/smartRPA/p-50_100_experiment/\"\n",
    "\n",
    "\n",
    "# DO NOT Change from here for synthetic data!\n",
    "# ---- Columns to generate the validation data for the experiment ----\n",
    "validationDataColumns = [\"uiLogName\",\"variationPercentage\",\"numberOfOccurrancesToBeDiscovered\",\"motifLength\",\"percentageMotifsOverLog\",\"logLength\",\n",
    "                     \"motifSpots\",\"caseIds\"]\n",
    "validationDataDF = pd.DataFrame(columns=validationDataColumns)\n",
    "concept_name_column = 'case:concept:name'\n",
    "timeStampCol = \"time:timestamp\"\n",
    "reduce = 0 # Currently not in use\n",
    "\n",
    "# ---- Gathering of unique events until the upper limit is reached ----\n",
    "dfAll = read_csvs_and_combine(csvPath,1000000)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721          \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df_unique = dfAll.drop_duplicates(subset=subset)\n",
    "print(f\"There are {df_unique.shape[0]} unique events in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 motifs have been generated. Proceeding with log creation.\n",
      "UI Log with name t-Baseline-LenLog_1_vin0_no1_ro10_rlen25_p100_len250.csv completed and stored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_21108\\984483377.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "validationDataDF = pd.DataFrame(columns=validationDataColumns)\n",
    "# ---- Loops to create the data and store the file containing the data parameters ----\n",
    "for mot in motifs: # Currently has no effect on the creation >> Would only be two different logs created \n",
    "    for rand in randomness:\n",
    "        for occ in occurances:\n",
    "            for percentage in percentageMotifsOverLog:\n",
    "                for motifLen in lengthMotifs:\n",
    "                    # Calculate the length of the log based on the percentage values\n",
    "                    \n",
    "                    l = ((occ*motifLen) / percentage * 100)\n",
    "                    # print(f\"Log Length: {l}\")\n",
    "                    samplingLength = l-(occ*motifLen)\n",
    "                    # print(f\"Random Events: {samplingLength}\")\n",
    "                    # Check if multiple motifs should be inserted\n",
    "                    if mot >= 1:\n",
    "                        random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs)) # Mot > 2\n",
    "                        filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "                        trimmed_df = pd.DataFrame()\n",
    "                        for case in random_cases_list:\n",
    "                            case_data = dfAll[dfAll[concept_name_column] == case].iloc[:motifLen]\n",
    "                            trimmed_df = pd.concat([trimmed_df, case_data], ignore_index=True)\n",
    "                        filtered_df = trimmed_df\n",
    "                    else:\n",
    "                        print(f\"Error: Cannot generate {mot} motifs. Skipping.\")\n",
    "                        break\n",
    "                    print(f\"{mot} motifs have been generated. Proceeding with log creation.\")\n",
    "                    # Sample the UI log from all available data\n",
    "                    if samplingLength > 0:\n",
    "                        uiLog = get_rand_uiLog(df_unique, n_max=rand, actions=int(samplingLength)) # set to actions=l for proper work\n",
    "                        print(f\"UI Log of length {l} is sampled. Proceeding with motif insertion.\")\n",
    "                        \n",
    "                        for shuffle in shuffles:\n",
    "                            if percentage == 2.5:\n",
    "                                filename = f\"Baseline-LenLog_{rand}_vin{shuffle}_no{mot}_ro{occ}_rlen{motifLen}_p2-5_len{int(l)}.csv\"\n",
    "                            else:\n",
    "                                filename = f\"Baseline-LenLog_{rand}_vin{shuffle}_no{mot}_ro{occ}_rlen{motifLen}_p{percentage}_len{int(l)}.csv\"\n",
    "                            # random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "                            # filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)] # Needs adjustment if two motifs should be inserted\n",
    "                            # filtered_df = filtered_df.iloc[:motifLen]\n",
    "                            \n",
    "                            uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                                uiLog=uiLog,\n",
    "                                                                dfcases=filtered_df,\n",
    "                                                                occurances=occ,\n",
    "                                                                case_column_name=concept_name_column,\n",
    "                                                                sorted_insert_col=timeStampCol,\n",
    "                                                                shuffled=True,\n",
    "                                                                shuffled_by=shuffle+1,\n",
    "                                                                reduced=False,\n",
    "                                                                reduced_by=reduce)\n",
    "                            \n",
    "                    else: # This is buggy at the moment. Does produce too long logs.\n",
    "                        # random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "                        # filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)] # Needs adjustment if two motifs should be inserted\n",
    "                        # filtered_df = filtered_df.iloc[:motifLen] # Needs adjustment if two motifs should be inserted\n",
    "                        for shuffle in shuffles: \n",
    "                            filename = f\"t-Baseline-LenLog_{rand}_vin{shuffle}_no{mot}_ro{occ}_rlen{motifLen}_p{percentage}_len{int(l)}.csv\"\n",
    "                            if len(random_cases_list) >= 2:\n",
    "                                sampled_dataframes = list()   \n",
    "                                inserted_cases_list = []               \n",
    "                                for _ in range(occ):\n",
    "                                    random_case = random.choice(random_cases_list)\n",
    "                                    inserted_cases_list.append(random_case) # Must be double checked Edited afterwards to add case ids.\n",
    "                                    sampled_data = dfAll[dfAll[concept_name_column] == random_case]\n",
    "                                    sampled_data = sampled_data[:motifLen]\n",
    "                                    sampled_dataframes.append(sampled_data)\n",
    "                                uiLog = pd.concat(sampled_dataframes, ignore_index=True)\n",
    "                            elif len(random_cases_list) == 1: # Only one motif is expected\n",
    "                                case = dfAll[dfAll[concept_name_column] == random_cases_list[0]]\n",
    "                                inserted_cases_list = \"[\" + \",\".join([str(x) for x in np.repeat(random_cases_list[0], occ)]) + \"]\"\n",
    "                                case = case[:motifLen]\n",
    "                                uiLog = pd.concat([case] * occ, ignore_index=True)\n",
    "                            else:\n",
    "                                print(f\"Cannot generate negative length or zero routines dataframe: {filename}\")\n",
    "                                break\n",
    "                            indices = [i * motifLen for i in range(occ)]\n",
    "                            random_cases_list = inserted_cases_list\n",
    "\n",
    "                    new_row = {'uiLogName': filename, \"variationPercentage\":  shuffle, \"motifsToBeDiscovered\": mot, \"numberOfOccurrancesToBeDiscovered\": occ,\n",
    "                    \"motifLength\": motifLen, \"percentageMotifsOverLog\": percentage, \"logLength\": l, \"motifSpots\": indices, \"caseIds\": random_cases_list}\n",
    "                    filepath = validation_path + filename\n",
    "                    uiLog.to_csv(filepath, index=False)\n",
    "                    print(f\"UI Log with name {filename} completed and stored\")\n",
    "                    # It is know that there is a FutureWarning for all-na or empty rows. Based on the previous logic everythin will work.\n",
    "                    validationDataDF = validationDataDF._append(new_row, ignore_index=True)\n",
    "\n",
    "validationDataDF.to_csv(validation_path + \"baseline_validation_mot2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive - Not in Use\n",
    "\n",
    "### Creation of artificial validation logs for TSDM Discovery in UI logs\n",
    "\n",
    "Method:\n",
    "  1. Get user interactions (a) and create a set of user actions (A)\n",
    "  2. Select random actions (1 to n consequtive actions per looping) append them into a dataframe (D) until a upper limit (x) is reached\n",
    "        - The upper limit x is considered as 1 action per 3 seconds in a 8 hour work day => 8* 60 * (60/3) = 9600 actions a day\n",
    "  4. Get routines (r) (1-m overall) and insert the routines (r) o-times at random points into the dataframe (D)\n",
    "        - The routines need not interrupt themselfs, otherwise no motif could be discovered (for future tests, the could interrupt as well)\n",
    "\n",
    "Result: A dataframe (D) with x + (o * len(r)) number of actions containing m routines at random points\n",
    "\n",
    "We create a set of UI logs which vary in the following constraints:\n",
    "- Randomness of actions inserted into the logs\n",
    "    - The actions can be sampled completly randomly\n",
    "    - The actions can be consequitive actions taken from previous logs\n",
    "- Number of motifs inserted\n",
    "    - The length of the motif is defined by the original motif in the smartRPA log\n",
    "- Number of times the motifs are inserted\n",
    "- Length of the UI log\n",
    "\n",
    "Objective: Create a dataframe that mimics a long time recording of users, which contains routines\n",
    "\n",
    "The file name will be created as follows\n",
    "\"Log_ [#Randomness]_ [#Motifs] _[#MotifOccurance] _[#LogLength]_[#ShuffledBy]_[#ReducedBy].csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Set the Data Path for the csv files used for the data sampling and where the logs should be added ----\n",
    "csvPath = \"logs/smartRPA/\"\n",
    "validation_path = csvPath + \"validation/\"\n",
    "\n",
    "randomness = [1] # how long should the original action sequences be\n",
    "motifs = [1] # how many different motifs should be inserted into the log\n",
    "occurances = [10,15,20,30,60] #motifs count of the motif occurances in the log\n",
    "lengthLog = [1000,2000,4000,8000,12000,15000,17500,20000,25000,30000] # 9600 are Events for approximatly one working day\n",
    "percentageMotifsOverLog = [1,2.5,5,10]\n",
    "\n",
    "# ToDo or just take the original size of the case to make it more real world\n",
    "lengthMotifs = [5,10,15,20,25] # Could be added to enter different length motifs into the data\n",
    "\n",
    "# Shuffle and reduction of the event log\n",
    "shuffle = 15\n",
    "reduce = 15\n",
    "\n",
    "concept_name_column = 'case:concept:name'\n",
    "timeStampCol = \"time:timestamp\"\n",
    "\n",
    "dfAll = read_csvs_and_combine(csvPath,1000000)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721          \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df_unique = dfAll.drop_duplicates(subset=subset)\n",
    "print(f\"There are {df_unique.shape[0]} unique events in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': []}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    # Not used at the moment as we have changed to percentage based calculation\n",
    "    for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "        beginningLog = get_rand_uiLog(dfAll, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "        for occ in occurances:\n",
    "            for mot in motifs: # Number of motifs\n",
    "                uiLog = beginningLog\n",
    "                random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=15)\n",
    "                # Filter rows with values in the list, because the length is shorter for the following loop\n",
    "                filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "                uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                           uiLog=uiLog,\n",
    "                                                           dfcases=filtered_df,\n",
    "                                                           occurances=occ,\n",
    "                                                           case_column_name=concept_name_column,\n",
    "                                                           sorted_insert_col=timeStampCol,\n",
    "                                                           shuffled=False,\n",
    "                                                           shuffled_by=shuffle,\n",
    "                                                           reduced=False,\n",
    "                                                           reduced_by=reduce)\n",
    "                filename = f\"Log_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}_.csv\"\n",
    "                filepath = validation_path + filename\n",
    "                print(filename)\n",
    "                uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "                # For tracking purpose and validation: Store the index of the cases in each log\n",
    "                row = (filename, str(indices), str(random_cases_list)) \n",
    "                new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "                index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"validation_data_high_percentage.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously part created motifs have all the same length. However, the data in our approach can handle different length motifs as well.\n",
    "We use the work in doi.org/10.1109/ACCESS.2023.3295995 to identif such motifs.\n",
    "The logs created here contain different length sized motifs, resembling different length user tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Filename': [], 'Index': [], 'CaseOrder': [], 'CaseLength':[]}\n",
    "index_frame = pd.DataFrame(data)\n",
    "for rand in randomness:\n",
    "    for occ in occurances:\n",
    "        for l in lengthLog: # do not name len as shortcut, cause trouble with inbuild length function len()\n",
    "            uiLog = get_rand_uiLog(df_unique, n_max=rand, actions=l) # set to actions=l for proper work\n",
    "            mot = len(lengthMotifs)\n",
    "            random_cases_list = get_random_values(dfAll, concept_name_column, mot, min_len=max(lengthMotifs))\n",
    "            filtered_df = dfAll[dfAll[concept_name_column].isin(random_cases_list)]\n",
    "\n",
    "            # Reduce the cases in length and append again\n",
    "            strCaseLength = \"\"\n",
    "            for i, element in enumerate(lengthMotifs):\n",
    "                insert_df = filtered_df[filtered_df[concept_name_column] == random_cases_list[i]].sort_values(timeStampCol)\n",
    "                try:\n",
    "                    variableLengthDf = pd.concat([insert_df.iloc[:element], variableLengthDf], ignore_index=True)\n",
    "                except NameError:\n",
    "                    variableLengthDf = insert_df.iloc[:element-1]\n",
    "                strCaseLength = strCaseLength + f\"{random_cases_list[i]}:{element}/\"\n",
    "\n",
    "            uiLog, indices, random_cases_list = insert_motifs_non_overlap(random_cases_list=random_cases_list,\n",
    "                                                        uiLog=uiLog,\n",
    "                                                        dfcases=variableLengthDf,\n",
    "                                                        occurances=occ,\n",
    "                                                        case_column_name=concept_name_column,\n",
    "                                                        sorted_insert_col=timeStampCol,\n",
    "                                                        shuffled=True,\n",
    "                                                        shuffled_by=shuffle,\n",
    "                                                        reduced=True,\n",
    "                                                        reduced_by=reduce)\n",
    "        \n",
    "            filename = f\"VarLenLog_{rand}_{mot}_{occ}_{l}_{shuffle}_{reduce}.csv\"\n",
    "            filepath = validation_path + filename\n",
    "            uiLog.to_csv(filepath, index=False)\n",
    "\n",
    "            # For tracking purpose and validation: Store the index of the cases in each log\n",
    "            row = (filename, str(indices), str(random_cases_list), strCaseLength) \n",
    "            new_row_series = pd.Series(row, index=index_frame.columns)\n",
    "            index_frame = pd.concat([index_frame, new_row_series.to_frame().T], ignore_index=True)\n",
    "\n",
    "filepath = validation_path + \"var_len_validation_data.csv\"\n",
    "index_frame.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeSeriesData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
