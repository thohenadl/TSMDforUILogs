{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9aa7d02-1b81-407f-a4a9-fcc94df4bdda",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ebe4c9-d293-48fa-b5be-3c6e2be65ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stumpy\n",
    "import numpy as np\n",
    "\n",
    "import time # just for dev purpose\n",
    "from IPython.display import display # Just for displaying DF nicely\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime as dt\n",
    "\n",
    "from util.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfd727-340e-4420-afb7-e587b57ff8ba",
   "metadata": {},
   "source": [
    "### Reading only one file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9e734",
   "metadata": {},
   "source": [
    "Agostinelli Logs (run this or Sparkasse Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044766f4-e957-4462-a320-36b4233c0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Logs\n",
    "csvPath = \"logs/smartRPA/\"\n",
    "\n",
    "filename = \"2020-11-07_17-28-29__250_25_40_3.csv\"\n",
    "#filename = \"2020-11-07_17-41-57__1000_100_80_4.csv\"\n",
    "#filename = \"2020-11-07_21-09-26__750_100_80_5.csv\"\n",
    "#filename = \"2020-11-17_15-05-40__500_100_40_4.csv\"\n",
    "\n",
    "filenamesList = [\"2020-11-17_15-05-40__500_100_40_4.csv\",\"2020-11-07_21-09-26__750_100_80_5.csv\",\n",
    "                 \"2020-11-07_17-41-57__1000_100_80_4.csv\",\"2020-11-07_17-28-29__250_25_40_3.csv\"]\n",
    "\n",
    "df = pd.read_csv(csvPath + filename, encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cac5e5",
   "metadata": {},
   "source": [
    "Sparkasse 1h Recording Sample by Tom (run this or Agostinelli Logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparkasse 1h test log\n",
    "csvPath = \"logs/Banking/\"\n",
    "\n",
    "filename = \"TSMD Log Sparkass_combined.csv\"\n",
    "df = pd.read_csv(csvPath + filename, encoding = \"latin-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72da7d-7013-4a20-aced-467b9b2e99a8",
   "metadata": {},
   "source": [
    "### Creating a mix DF from multiple files\n",
    "\n",
    "This process creates a DF using cases from different files that are within the filenameList.\n",
    "\n",
    "For each file\n",
    "- Select 5 unique case ids\n",
    "- Get all events per case\n",
    "- Append these events to the mixed_df\n",
    "\n",
    "Result: (5 * Number of Files) Cases in a new DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0bdf0-2c4c-462f-b23d-62ebd2173231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df containing cases from different Agostinelli logs to vary patterns\n",
    "mixed_df = pd.DataFrame() \n",
    "for file in filenamesList:\n",
    "    df = pd.read_csv(csvPath + file)\n",
    "    # Get the first 5 unique case:concept:name values\n",
    "    unique_cases = df['case:concept:name'].unique()[:5]\n",
    "    \n",
    "    # Filter the DataFrame to keep only rows with those unique values\n",
    "    df_filtered = df[df['case:concept:name'].isin(unique_cases)]\n",
    "    concat_dfs = [mixed_df,df_filtered]\n",
    "    mixed_df = pd.concat(concat_dfs,ignore_index=True)\n",
    "\n",
    "df = mixed_df\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b7b5d-1520-4634-b495-300ca7dd52e3",
   "metadata": {},
   "source": [
    "### Creating a validation data set from available logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50ed3f-6907-49c4-845f-8d2a5c479c24",
   "metadata": {},
   "source": [
    "Objective: Create a dataframe that mimics a long time recording of users, which contains routines\n",
    "\n",
    "Method:\n",
    "  1. Get user interactions (a) and create a set of user actions (A)\n",
    "  2. Select random actions (1-n consequtive per looping) append them into a dataframe (D) until a upper limit (x) is reached\n",
    "        - The upper limit x is considered as 1 action per 3 seconds in a 8 hour work day => 8* 60 * (60/3) = 9600 actions a day\n",
    "  4. Get routines (r) (1-m overall) and insert the routines (r) o-times at random points into the dataframe (D)\n",
    "        - The routines need not interrupt themselfs, otherwise no motif could be discovered (for future tests, the could interrupt as well)\n",
    "\n",
    "Result: A dataframe (D) with x + (o * len(r)) number of actions containing m routines at random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1ec13-a4e1-47e4-a136-789d3c01aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# ---- 1 Get user interactions ----\n",
    "df = read_csvs_and_combine(csvPath)\n",
    "# Drop duplicates based on equality assumption in https://doi.org/10.1016/j.compind.2022.103721                                                                                     \n",
    "subset=[\"category\",\"application\",\"concept:name\",\"event_src_path\",\"event_dest_path\",\"browser_url\",\"xpath\"]\n",
    "df = df.drop_duplicates(subset=subset)\n",
    "print(f\"There are {len(df)} unique events in the dataframe\")\n",
    "\n",
    "time2 = time.time()\n",
    "print(f\"Row creation took {time2-start_time} \\n\")\n",
    "\n",
    "# ---- 2 Create shuffled UI log ----\n",
    "uiLog = get_rand_uiLog(df)\n",
    "\n",
    "time3 = time.time()\n",
    "print(f\"UiLog creation took {time3-time2} \\n\")\n",
    "\n",
    "# ---- 3 Add routines in the dataframe ----\n",
    "column_name = 'case:concept:name'\n",
    "m = 1\n",
    "o = 3\n",
    "random_list = get_random_values(df, column_name, m, 15)\n",
    "\n",
    "routine = df[df[column_name] == random_list[0]]\n",
    "modified_df, indices = insert_rows_at_random(uiLog, routine, o, shuffled=False, reduced=False, reduced_by=20)\n",
    "\n",
    "# ---- 4 Outputting stuff ----\n",
    "end_time = time.time()\n",
    "print(f\"Random case insertion took {end_time-time3} \\n\")\n",
    "print(random_list)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3f53d-0411-4358-9908-f968e417fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = modified_df\n",
    "df[df[\"case:concept:name\"]==random_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969efaf3",
   "metadata": {},
   "source": [
    "# Adding Boundary Information \n",
    "Utilizing the method proposed by Rebmann and van der Aa (https://link.springer.com/chapter/10.1007/978-3-031-34560-9_9) in https://github.com/a-rebmann/task-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447f92a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>micro_task</th>\n",
       "      <th>timeDifferenceBoolStatic</th>\n",
       "      <th>timeDifferenceBoolRolling</th>\n",
       "      <th>isBoundary</th>\n",
       "      <th>timeDifference</th>\n",
       "      <th>n-running-difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6246</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>25.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6247</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6248</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "      <td>25.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6250 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     micro_task  timeDifferenceBoolStatic  timeDifferenceBoolRolling  \\\n",
       "0          True                     False                      False   \n",
       "1                                   False                      False   \n",
       "2                                   False                      False   \n",
       "3                                   False                      False   \n",
       "4                                   False                      False   \n",
       "...         ...                       ...                        ...   \n",
       "6245                                False                       True   \n",
       "6246                                False                      False   \n",
       "6247                                False                      False   \n",
       "6248                                False                       True   \n",
       "6249       True                     False                      False   \n",
       "\n",
       "      isBoundary  timeDifference  n-running-difference  \n",
       "0           True              27                   NaN  \n",
       "1          False              55                   NaN  \n",
       "2          False              28                   NaN  \n",
       "3          False              26                   NaN  \n",
       "4          False              60                   NaN  \n",
       "...          ...             ...                   ...  \n",
       "6245        True              30                  26.7  \n",
       "6246       False              12                  25.4  \n",
       "6247       False              25                  25.2  \n",
       "6248        True              43                  25.4  \n",
       "6249        True               0                  25.2  \n",
       "\n",
       "[6250 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.uipatternminer import UIPatternMiner\n",
    "from util.const import path_to_files, log_dir, value_attributes, semantic_attributes, context_attributes, MICROTASK, CASEID, USERACTIONID, case_ids, timeStamps\n",
    "\n",
    "# 2. Get Micro Tasks by creating uipatternminer Class Object\n",
    "context_atts = context_attributes\n",
    "value_atts = value_attributes\n",
    "semantic_atts = semantic_attributes\n",
    "\n",
    "for file in filenamesList:\n",
    "    case_ids = {file: \"case:concept:name\"}\n",
    "    timeStamps = {file: \"time:timestamp\"}\n",
    "\n",
    "miner = UIPatternMiner(df, case_ids, timeStamps, context_atts, value_atts, semantic_atts)\n",
    "miner.get_micro_tasks()\n",
    "boundary_indices = miner.get_attribute(\"boundaries\")\n",
    "\n",
    "# Add column with boundaries to df\n",
    "df[MICROTASK] = ''\n",
    "for index in boundary_indices:\n",
    "    df.at[index, MICROTASK] = 'True'\n",
    "\n",
    "# Include calculate_time_difference \n",
    "# Adding a weekly average as seperator (gap attribute) might be a solution to get better \"segmenetation\" of task switches as described in doi.org/10.1145/2063576.2063947\n",
    "df = calculate_time_difference(df, miner, gap=300, n_rolling=10)\n",
    "\n",
    "# Merge case boundary col, time Difference Boundary, and micro task boundary col in isBoundaryCol\n",
    "df['isBoundary'] = df.apply(lambda row: row[MICROTASK] == 'True' or row['timeDifferenceBoolStatic'] == True or row['timeDifferenceBoolRolling'] == True, axis=1)\n",
    "\n",
    "df[[MICROTASK,\"timeDifferenceBoolStatic\",\"timeDifferenceBoolRolling\",'isBoundary','timeDifference',\"n-running-difference\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cfd4e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isBoundary\n",
      "False    3079\n",
      "True     3171\n",
      "Name: count, dtype: int64\n",
      "6250\n"
     ]
    }
   ],
   "source": [
    "result = df.groupby('isBoundary')['isBoundary'].value_counts()\n",
    "print(result)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405156e4-a750-4a56-a804-6a0b452de2fd",
   "metadata": {},
   "source": [
    "### Getting the tuple data ready\n",
    "#### Agostinelli Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c4dbc-b66c-435e-b81c-05db4d4bbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptNames = {\n",
    "    'beforeSaveWorkbook','urlHashChange','contextMenu','clickCheckboxButton','clickRadioButton','navigateTo','link','typed','form','reload','clickTextField',\n",
    "    'clickButton','clickLink','selectOptions','selectText','submit','changeField','doubleClick','dragElement','cancelDialog','fullscreen','attachTab',\n",
    "    'detachTab','newBookmark','removeBookmark','modifyBookmark','moveBookmark','startDownload','erasedDownload','installBrowserExtension','uninstallBrowserExtension',\n",
    "    'enableBrowserExtension','disableBrowserExtension','closedNotification','clickedNotification','newWindow','closeWindow','newTab','closeTab','moveTab',\n",
    "    'mutedTab','unmutedTab','pinnedTab','unpinnedTab','audibleTab','zoomTab','changeHistory','created','modified','deleted','Mount','Unmount','moved',\n",
    "    'programOpen','programClose','selectFile','selectFolder','hotkey','insertUSB','printSubmitted','openFile','openFolder','copy','paste','cut','openWindow','closeWindow',\n",
    "    'resizeWindow','newWorkbook','openWorkbook','addWorksheet','saveWorkbook','printWorkbook','closeWorkbook','activateWorkbook','deactivateWorkbook','modelChangeWorkbook',\n",
    "    'newChartWorkbook','afterCalculate','selectWorksheet','deleteWorksheet','doubleClickCellWithValue','doubleClickEmptyCell','rightClickCellWithValue',\n",
    "    'rightClickEmptyCell','sheetCalculate','editCellSheet','deselectWorksheet','followHiperlinkSheet','pivotTableValueChangeSheet','getRange',\n",
    "    'getCell','worksheetTableUpdated','addinInstalledWorkbook','addinUninstalledWorkbook','XMLImportWorkbook','XMLExportWorkbook','activateWindow',\n",
    "    'deactivateWindow','doubleClickWindow','rightClickWindow','newDocument','openDocument','changeDocument','saveDocument','printDocument','activateWindow',\n",
    "    'deactivateWindow','rightClickPresentation','doubleClickPresentation','newPresentation','newPresentationSlide','closePresentation','savePresentation',\n",
    "    'openPresentation','printPresentation','slideshowBegin','nextSlideshow','clickNextSlideshow','previousSlideshow','slideshowEnd','SlideSelectionChanged',\n",
    "    'startupOutlook','quitOutlook','receiveMail','sendMail','logonComplete','newReminder'\n",
    "    }\n",
    "\n",
    "# Update dict creation to sort the data before to create meaningful dicts -> Equal applications are grouped together for example\n",
    "conceptNamesDict = createDict(conceptNames)\n",
    "conceptNamesDict = createDict(set(df.sort_values(by=['concept:name'])['concept:name'].unique()))\n",
    "# applicationDict = createDict(set(df.sort_values(by=['application'])['application'].unique()))\n",
    "applicationDict = createDict(set(df['application'].unique()))\n",
    "categoriesDict = createDict(set(df.sort_values(by=['category'])['category'].unique()))\n",
    "\n",
    "print(f\"The number of concept names is {len(conceptNamesDict)}, of applications is {len(applicationDict)}, and categories is {len(categoriesDict)}\")\n",
    "\n",
    "# Add new columns with IDs (corrected call to apply)\n",
    "# Agostinelli Data\n",
    "df['concept:name:id'] = df.apply(lambda row: get_key(row, conceptNamesDict, 'concept:name'), axis=1)\n",
    "df['application:id'] = df.apply(lambda row: get_key(row, applicationDict, 'application'), axis=1)\n",
    "df['category:id'] = df.apply(lambda row: get_key(row, categoriesDict, 'category'), axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "# For Agostinelli Data\n",
    "print(df[[\"time:timestamp\",'concept:name', 'application', 'category', 'concept:name:id', 'application:id', 'category:id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5357c",
   "metadata": {},
   "source": [
    "Make the multi dimensional Agostinelli log one dimensional using another tuple generator\n",
    "\n",
    "Using the ideas ('concept:name:id','application:id', 'category:id') generated earlier in the process based on the dictonaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f554037-4a5a-4dff-a8be-ee742612fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbersDF = df[['concept:name:id', 'application:id', 'category:id']]\n",
    "\n",
    "# Generate unique tuples for indexing the individual combinations of the rows mentioned\n",
    "unique_df = numbersDF.drop_duplicates(subset=numbersDF.columns, keep='first')\n",
    "tuples = [tuple(row[['concept:name:id', 'application:id', 'category:id']]) for i, row in unique_df.sort_values(by='application:id').iterrows()]\n",
    "       \n",
    "\n",
    "df['id'] = df.apply(lambda row: get_id(row, tuples, columns=['concept:name:id','application:id', 'category:id']), axis=1)\n",
    "# Print the updated DataFrame\n",
    "print(df['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3798365a",
   "metadata": {},
   "source": [
    "### Display different ordering clusters\n",
    "Timeseries data is continuous and thus the process mining data has to be transformed in a way that it does represent a continuous spectrum of values.\n",
    "\n",
    "We test the following approaches to create the timeseries integer values for a continuous spectrum:\n",
    "- Tuple ID by occurance: The first occurance gets ID=1, the second ID=2, and so on. This leads to a spectrum based on value occurance\n",
    "- Tuple ID by application: Similar applications should get similar tuple IDs. Thus, we define a list of applications that can occurr in the log and order the applications by similarity. E.g. Firefox and Chrome have closer tuple values compared to Firefox and Outlook.\n",
    "- Tuple ID by action: Similar actions get similar IDs. Thus, we cluster the actions based on their generic type. We suggest the clustering based on:\n",
    "    - mouse interactions, keyboard interactions, and other interactions.\n",
    "    - the generic action types \"Open\" \"Close\" \"Navigate\" \"Transform\" \"Transfer\" \"Empty\" \"Conclude\"\n",
    "\n",
    "Based on the sorting we check the quality of the data distribution by the mean difference between the tuple IDs in the log.\n",
    "I.e. we calculate the difference between all occurring values in the log and take the mechanism creating the smallest average difference between consequitive occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeec816",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_50_rows = df.iloc[:50]\n",
    "\n",
    "# Get row IDs (index starts from 0)\n",
    "row_ids = first_50_rows.index\n",
    "\n",
    "# Extract application values\n",
    "application_values = first_50_rows['application:id'].to_numpy()  # Convert to NumPy array for efficiency\n",
    "concept_names = first_50_rows['concept:name:id'].to_numpy()  \n",
    "categories_values = first_50_rows['category:id'].to_numpy()  \n",
    "tuple_ids = first_50_rows['id'].to_numpy() \n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(row_ids, application_values, color='blue')\n",
    "plt.scatter(row_ids, concept_names, color='red')\n",
    "plt.scatter(row_ids, categories_values, color='grey')\n",
    "plt.scatter(row_ids, tuple_ids, color='orange')\n",
    "plt.xlabel('Row ID')\n",
    "plt.ylabel('Application Value')\n",
    "plt.title('Row ID vs. Application Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences between consecutive values (avoid first element)\n",
    "differences_app = df['application:id'].diff(periods=-1)  # Avoids unnecessary row iteration\n",
    "differences_app.iloc[0] = pd.NA  # Set difference for the first row to not-a-number (optional)\n",
    "\n",
    "differences_concept = df['concept:name:id'].diff(periods=-1) \n",
    "differences_concept.iloc[0] = pd.NA  \n",
    "\n",
    "differences_cat = df['category:id'].diff(periods=-1)  \n",
    "differences_cat.iloc[0] = pd.NA  \n",
    "\n",
    "differences_tuple = df['id'].diff(periods=-1)  \n",
    "differences_tuple.iloc[0] = pd.NA\n",
    "\n",
    "# Calculate the mean of absolute differences (mean jump)\n",
    "mean_jump_app = differences_app.abs().mean()\n",
    "mean_jump_conceptname = differences_concept.abs().mean()\n",
    "mean_jump_cat = differences_cat.abs().mean()\n",
    "mean_jump_tup = differences_tuple.abs().mean()\n",
    "\n",
    "print(\"Mean alteration rate between consecutive application:id values:\", mean_jump_app) # Best score so far when just taking apps as they come at 1.8\n",
    "print(\"Mean alteration rate between consecutive concept:name:id values:\", mean_jump_conceptname) # best score if only concepts are considered that appear in the df at 12.6 (if all actions are considered ~36)\n",
    "print(\"Mean alteration rate between consecutive category:id values:\", mean_jump_cat) \n",
    "print(\"Mean alteration rate between consecutive tuple id values:\", mean_jump_tup) # score does not change a lot, best score around 9.3/9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec0050-7a52-4ebc-8f46-5233d3482803",
   "metadata": {},
   "source": [
    "Modelling the data in a 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab770ccd-df70-41fe-a4c4-a7c853a336dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extract data from each column\n",
    "x = df['concept:name:id']\n",
    "y = df['application:id']\n",
    "z = df['category:id']\n",
    "\n",
    "# Plot the points\n",
    "ax.scatter(x, y, z, c='blue', alpha=0.7)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('concept:name:id')\n",
    "ax.set_ylabel('application:id')\n",
    "ax.set_zlabel('category:id')\n",
    "ax.set_title('3D Model from DataFrame')\n",
    "\n",
    "# Adjust view angles (optional)\n",
    "ax.view_init(elev=15, azim=60)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30be16-30fc-43e4-9145-30272347ee04",
   "metadata": {},
   "source": [
    "## Motife Discovery\n",
    "\n",
    "From Time https://stumpy.readthedocs.io/en/latest/Tutorial_STUMPY_Basics.html#Find-a-Motif-Using-STUMP:\n",
    "\n",
    "Time series motifs are approximately repeated subsequences found within a longer time series. Being able to say that a subsequence is “approximately repeated” requires that you be able to compare subsequences to each other. In the case of STUMPY, all subsequences within a time series can be compared by computing the pairwise z-normalized Euclidean distances and then storing only the index to its nearest neighbor. This nearest neighbor distance vector is referred to as the matrix profile and the index to each nearest neighbor within the time series is referred to as the matrix profile index. Luckily, the stump function takes in any time series (with floating point values) and computes the matrix profile along with the matrix profile indices and, in turn, one can immediately find time series motifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd878e4e",
   "metadata": {},
   "source": [
    "### Window Size Calculation\n",
    "\n",
    "I propose a window size calculation based on break time. \n",
    "The break time method (windowSizeByBreak) calculates the percentil break time and then the avg. number of events between these breaks occuring.\n",
    "\n",
    "E.g. the avg. break time is 300 seconds and the following breaks between event time stamps exist:\n",
    "[100,210,333,100,11,300,222] \n",
    "\n",
    "Then there are on avg. 3 events between these breaks: Thus, the window size is set to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the percentil is increased the number of elements should increate, but it does not, it decreases\n",
    "# Something is wrong with this functionallity\n",
    "# Occarm´s Razor: The most simple solution might be the best: Just take a guess instead of \"Rätselraten\"\n",
    "percentil = 75\n",
    "breakTime = 300 # 300 = 5 Min, 600 = 10 min, 900 = 15 min\n",
    "third_quartile, quartile_indices, average_elements = windowSizeByBreak(df, \"time:timestamp\", breakTime, percentil)\n",
    "if average_elements is not None:\n",
    "    print(f\"Average number of elements between {percentil} percentil breaks occurrences: {average_elements:.2f}\")\n",
    "else:\n",
    "    print(\"Not enough data to calculate average\")\n",
    "\n",
    "print(f\"The {percentil} percentil time difference between clicks is {third_quartile}s\")\n",
    "\n",
    "# Checking the avg. case length:\n",
    "# Count occurrences of each concept name\n",
    "concept_counts = df['case:concept:name'].value_counts()\n",
    "average_entries = concept_counts.mean()\n",
    "print(f\"The average case length is {average_entries} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80ad83-6224-4f6b-ad7c-ecc36ecd8b9c",
   "metadata": {},
   "source": [
    "### Run for smartRPA based File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d04a2",
   "metadata": {},
   "source": [
    "Window size selection is challenging task for motif discovery as they occurr independent and often. Motifs can have varying length and thus be hard to find.\n",
    "More research is needed as stated in https://link.springer.com/chapter/10.1007/978-3-031-24378-3_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85056493-be8e-45ec-87e9-9ee2dc48813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_row = 0\n",
    "ending_row = len(df)-1\n",
    "#Extract ids and rows\n",
    "ids = df.loc[starting_row:ending_row,'id'].tolist()\n",
    "rows = [i for i in range(len(df.loc[starting_row:ending_row,'id']))]\n",
    "\n",
    "# Manually set window size\n",
    "window_size = 15\n",
    "\n",
    "event_series = df.loc[starting_row:ending_row,'id'].values.astype(float)\n",
    "tm_matrix = stumpy.stump(event_series, window_size)\n",
    "\n",
    "motif_idx_tm = np.argsort(tm_matrix[:, 0])[0]\n",
    "nearest_neighbor_idx_tm = tm_matrix[motif_idx_tm, 1]\n",
    "\n",
    "print(f\"The window size is {window_size}\")\n",
    "print(f\"The motif is located at index {motif_idx_tm}\")\n",
    "print(f\"The nearest neighbor is located at index {nearest_neighbor_idx_tm}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f10878-693c-46ff-adb8-1bfb977bc678",
   "metadata": {},
   "source": [
    "### Run for artificial file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74198916-a6a0-4e70-a371-054c6c6755a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_row = 0\n",
    "ending_row = len(df)-1\n",
    "#Extract ids and rows\n",
    "ids = df.loc[starting_row:ending_row,'id'].tolist()\n",
    "rows = [i for i in range(len(df.loc[starting_row:ending_row,'id']))]\n",
    "\n",
    "print(f\"The motif should be at {indices}\")\n",
    "for size in [5,10,15,20]:\n",
    "    window_size = size\n",
    "    \n",
    "    event_series = df.loc[starting_row:ending_row,'id'].values.astype(float)\n",
    "    tm_matrix = stumpy.stump(event_series, window_size)\n",
    "    \n",
    "    motif_idx_tm = np.argsort(tm_matrix[:, 0])[0]\n",
    "    nearest_neighbor_idx_tm = tm_matrix[motif_idx_tm, 1]\n",
    "\n",
    "    print(f\"The window size is {window_size}\")\n",
    "    print(f\"The motif is located at index {motif_idx_tm} and it is an artificial motif: {any(abs(motif_idx_tm - element) <= size for element in indices)}\")\n",
    "    print(f\"The nearest neighbor is located at index {nearest_neighbor_idx_tm}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbed2be-49d4-440e-bd84-284b6aa6fdc8",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7ada2-67c3-46f4-a817-8613751b0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, sharex=True, gridspec_kw={'hspace': 0}, figsize=(6.5, 4))\n",
    "plt.suptitle('Motif (Pattern) Discovery', fontsize='10')\n",
    "\n",
    "#Plot Event data\n",
    "axs[0].scatter(rows, ids, alpha=0.8)\n",
    "axs[0].set_ylabel('Events', fontsize='10')\n",
    "# Plot Timeseries data\n",
    "axs[1].plot(event_series)\n",
    "axs[1].set_ylabel('Timeseries', fontsize='10')\n",
    "rect = Rectangle((motif_idx_tm, 0), window_size, event_series.max(), facecolor='lightgrey')\n",
    "axs[1].add_patch(rect)\n",
    "rect = Rectangle((nearest_neighbor_idx_tm, 0), window_size, event_series.max(), facecolor='lightgrey')\n",
    "axs[1].add_patch(rect)\n",
    "# Plot Matrix profiles\n",
    "axs[2].set_xlabel('Events', fontsize ='10')\n",
    "axs[2].set_ylabel('Matrix Profile', fontsize='10')\n",
    "axs[2].set_ylim(top=tm_matrix[:, 0].max()*1.1) #displaying the max value with some uplift for space in Graph\n",
    "axs[2].plot(tm_matrix[:, 0])\n",
    "# Adding Dashed lines\n",
    "axs[0].axvline(x=motif_idx_tm, linestyle=\"dashed\", color='C1')\n",
    "axs[0].axvline(x=nearest_neighbor_idx_tm, linestyle=\"dashed\", color='C1')\n",
    "axs[1].axvline(x=motif_idx_tm, linestyle=\"dashed\", color='C1')\n",
    "axs[1].axvline(x=nearest_neighbor_idx_tm, linestyle=\"dashed\", color='C1')\n",
    "axs[2].axvline(x=motif_idx_tm, linestyle=\"dashed\", color='C1')\n",
    "axs[2].axvline(x=nearest_neighbor_idx_tm, linestyle=\"dashed\", color='C1')\n",
    "plt.show()\n",
    "\n",
    "# Display Pattern overlay\n",
    "fig, ax = plt.subplots(figsize=(6.5, 2))\n",
    "plt.title('Motif Overlay', fontsize='10')\n",
    "ax.set_xlabel(\"Events\", fontsize='10')\n",
    "ax.set_ylabel(\"Timeseries\", fontsize='10')\n",
    "# Plot motif and nearest neighbor window\n",
    "ax.plot(event_series[motif_idx_tm:motif_idx_tm+window_size], color='C1', label=\"Motif\")\n",
    "ax.plot(event_series[nearest_neighbor_idx_tm:nearest_neighbor_idx_tm+window_size], color='C2', label=\"Match\")\n",
    "plt.legend(loc=\"best\",fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the desired rows\n",
    "display(df.iloc[motif_idx_tm:min(motif_idx_tm + window_size, len(df))])\n",
    "display(df.iloc[nearest_neighbor_idx_tm:min(nearest_neighbor_idx_tm + window_size, len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd637a-28dd-42da-b0d6-db1eafa139e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events only plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Events', fontsize='10')\n",
    "ax.set_xlabel(\"Events\", fontsize='10')\n",
    "ax.set_ylabel(\"Tuple ID\", fontsize='10')\n",
    "ax.scatter(rows, ids, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4fff3-256c-45d5-b3ef-c26ce565f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Profil only\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Timeseries Events', fontsize='10')\n",
    "ax.set_xlabel(\"Timeseries\", fontsize='10')\n",
    "ax.set_ylabel(\"Tuple ID\", fontsize='10')\n",
    "ax.plot(event_series)\n",
    "# Plot motif and nearest neighbor window\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807f77e-61fd-4dad-a7d9-728f8b6de14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timeseries plot only\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Timeseries Events', fontsize='10')\n",
    "ax.set_xlabel(\"Timeseries\", fontsize='10')\n",
    "ax.set_ylabel(\"Tuple ID\", fontsize='10')\n",
    "ax.plot(tm_matrix[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d32e8-2797-4aa2-ab6a-6218df6e8e05",
   "metadata": {},
   "source": [
    "## Find Top-K Motifs\n",
    "\n",
    "From https://stumpy.readthedocs.io/en/latest/Tutorial_STUMPY_Basics.html#Find-Top-K-Motifs\n",
    "\n",
    "Now that you’ve computed the matrix profile, mp, for your time series and identified the best global motif, you may be interested in discovering other motifs within your data. However, you’ll immediately learn that doing something like top_10_motifs_idx = np.argsort(mp[:, 0])[10] doesn’t actually get you what you want and that’s because this only returns the index locations that are likely going to be close to the global motif! Instead, after identifying the best motif (i.e., the matrix profile location with the smallest value), you first need to exclude the local area (i.e., an exclusion zone) surrounding the motif pair by setting their matrix profile values to np.inf before searching for the next motif. Then, you’ll need to repeat the “exclude-and-search” process for each subsequent motif. Luckily, STUMPY offers two additional functions, namely, stumpy.motifs and stumpy.match, that help simplify this process. While it is beyond the scope of this basic tutorial, we encourage you to check them out!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbf6b3-ecb4-4912-98fb-e86e83d05123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: When a motif is discovered: Find all equal motifs in the data and extract (CTRL+X) them from the frame into a new frame\n",
    "#    this new frame contains all variants of a motif and could be input to smartRPAs routine variant discovery\n",
    "# Repeat until no motifs are found anymore\n",
    "\n",
    "# min_neighbours: With min_neighbours one can select how often a process motif has to appear at least to be relevant => map this to number of executions\n",
    "# max_matches\n",
    "top_motifs = stumpy.motifs(T=event_series, P=tm_matrix[:,0], min_neighbors=1, max_matches=10)\n",
    "top_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80f72a-0a0c-4498-9fd2-2bd2a4a52e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Event data\n",
    "fig2, axs2 = plt.subplots(3, sharex=True, gridspec_kw={'hspace': 0})\n",
    "plt.suptitle('Motif (Pattern) Discovery', fontsize='10')\n",
    "\n",
    "axs2[0].scatter(rows, ids, alpha=0.8)\n",
    "axs2[0].set_ylabel('Events', fontsize='10')\n",
    "# Plot Timeseries data\n",
    "axs2[1].plot(event_series)\n",
    "axs2[1].set_ylabel('Timeseries', fontsize='10')\n",
    "rect = Rectangle((motif_idx_tm, 0), window_size, event_series.max(), facecolor='lightgrey')\n",
    "# axs2[1].add_patch(rect)\n",
    "rect = Rectangle((nearest_neighbor_idx_tm, 0), window_size, event_series.max(), facecolor='lightgrey')\n",
    "# axs2[1].add_patch(rect)\n",
    "# Plot Matrix profiles\n",
    "axs2[2].set_xlabel('Time', fontsize ='10')\n",
    "axs2[2].set_ylabel('Matrix Profile', fontsize='10')\n",
    "axs2[2].set_ylim(top=tm_matrix[:, 0].max()*1.1) #displaying the max value with some uplift for space in Graph\n",
    "axs2[2].plot(tm_matrix[:, 0])\n",
    "# Adding Dashed lines\n",
    "for discovered in top_motifs[1][0]:\n",
    "    axs2[0].axvline(x=discovered, linestyle=\"dashed\",color='C1')\n",
    "    axs2[1].axvline(x=discovered, linestyle=\"dashed\",color='C1')\n",
    "    axs2[2].axvline(x=discovered, linestyle=\"dashed\",color='C1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245d3b0-ec34-4dd6-92e3-a5a8c8d5bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe that contains all paths from the top_motifs result \n",
    "allMotifsDF = pd.DataFrame() \n",
    "for i, element in enumerate(top_motifs[1][0]):\n",
    "    motif_start = top_motifs[1][0][i]\n",
    "    motifFrame = df.loc[motif_start:motif_start+window_size]\n",
    "    allMotifsDF = pd.concat([allMotifsDF,motifFrame])\n",
    "    print(motifFrame)\n",
    "\n",
    "allMotifsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24edd5e-6275-4931-9701-506356b65505",
   "metadata": {},
   "source": [
    "### Matching motives from most appearing motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031a4ff-ae7a-4c1c-8a86-14b9d981099a",
   "metadata": {},
   "source": [
    "Find all matches of a query `Q` in a time series `T`\n",
    "\n",
    "The indices of subsequences whose distances to `Q` are less than or equal to `max_distance`, sorted by distance (lowest to highest). Around each occurrence an exclusion zone is applied before searching for the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e79ff-15ec-42a5-bda1-f7f0fb0caadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstMotif = top_motifs[1][0][0]\n",
    "foundMotif = event_series[firstMotif:firstMotif+window_size]\n",
    "matchingPatterns = stumpy.match(Q=foundMotif,T=event_series,max_distance=0.5)\n",
    "print(firstMotif)\n",
    "print(matchingPatterns)\n",
    "print(f\"The amount of motifs found with matching patterns is {len(matchingPatterns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f42e35",
   "metadata": {},
   "source": [
    "## Identify process start and end\n",
    "\n",
    "1. It is necessary to identify if it is the same process that was identified.\n",
    "2. It is necessary to identify the start and end of the process:\n",
    "    Where is the initial action? What is an initial action? What is the last action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to the motif discovery by single motif discovery method\n",
    "index_to_check = motif_idx_tm\n",
    "end_of_window = index_to_check+window_size\n",
    "\n",
    "try:\n",
    "  start_forward_index, start_backward_index = find_closest_boundaries(df.copy(), index_to_check)\n",
    "  end_forward_index, end_backward_index = find_closest_boundaries(df.copy(), end_of_window)\n",
    "\n",
    "  print(f\"The actual motif start is at index {index_to_check} and ends at {index_to_check+window_size} with window size being {window_size}.\")\n",
    "  print(f\"Start Forward closest boundary index: {start_forward_index} and {abs(index_to_check-start_forward_index)} actions away.\")\n",
    "  print(f\"Start Backward closest boundary index: {start_backward_index} and {abs(index_to_check-start_backward_index)} actions away.\")  \n",
    "  print(f\"\\nEnd Forward closest boundary index: {end_forward_index} and {abs(end_of_window-end_forward_index)} actions away.\")\n",
    "  print(f\"End Backward closest boundary index: {end_backward_index} and {abs(end_of_window-end_backward_index)} actions away.\")  \n",
    "\n",
    "except ValueError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
