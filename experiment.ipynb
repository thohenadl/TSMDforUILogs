{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motif Discovery in User Interaction Logs \n",
    "\n",
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "import time # just for dev purpose\n",
    "from IPython.display import display # Just for displaying DF nicely\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime as dt\n",
    "\n",
    "from util.util import *\n",
    "from util.const import conceptNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "Read all files in the folder \"validation\" and the corresponding validation data containing the information about length and distribution of the motifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read validation_data.csv from the folder.\n",
      "[Errno 2] No such file or directory: 'logs/smartRPA/percentageComparison/validation_data.csv'\n",
      "Could not read var_len_validation_data.csv from the folder.\n",
      "[Errno 2] No such file or directory: 'logs/smartRPA/percentageComparison/var_len_validation_data.csv'\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"logs/smartRPA/validation/\"\n",
    "folder_path = \"logs/smartRPA/percentageComparison/\"\n",
    "\n",
    "UILogValidation_filename = \"validation_data.csv\"\n",
    "variableLenValidation_filename = \"var_len_validation_data.csv\"\n",
    "percentagData_filename = \"validationDataPercentage.csv\"\n",
    "\n",
    "varLenUILogs = []\n",
    "UILogs = []\n",
    "percentageLogs = []\n",
    "\n",
    "# Getting the relevant files from the folder and sorting them into different lists for processing\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.startswith(\"Log\"):\n",
    "        UILogs.append(file)\n",
    "    elif file.startswith(\"Var\"):\n",
    "        varLenUILogs.append(file)\n",
    "    elif file.startswith(\"LenLog\"):\n",
    "        percentageLogs.append(file)\n",
    "\n",
    "# Read the validation data into two dataframes for processing\n",
    "try:\n",
    "    UILogValidationDF = pd.read_csv(folder_path + UILogValidation_filename)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not read {UILogValidation_filename} from the folder.\\n{e}\")\n",
    "\n",
    "# Check if varliable motif data is present by checking if the var_len_validation_data.csv file exists\n",
    "try:\n",
    "    variableLenValidationDF = pd.read_csv(folder_path + variableLenValidation_filename)\n",
    "    varLenDataAvailable = True\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not read {variableLenValidation_filename} from the folder.\\n{e}\")\n",
    "    varLenDataAvailable = False\n",
    "\n",
    "# Check if data created for percentage based comparison is available\n",
    "try:\n",
    "    percentageValData = pd.read_csv(folder_path + percentagData_filename)\n",
    "    percentageValAvailable = True\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not read {percentagData_filename} from the folder.\\n{e}\")\n",
    "    percentageValAvailable = False\n",
    "\n",
    "experimentColumns = [\"experimentID\",\"uiLogName\",\"variationPercentage\",\"percentageMotifsOverLog\",\"motifLength\",\"windowSize\",\n",
    "                     \"windowSizeMatch\",\"motifsToBeDiscovered\",\"motifsDiscovered\",\"numberOfOccurrancesToBeDiscovered\",\n",
    "                     \"OccurancesDiscovered\",\"DiscoveryPercentage\",\"alignmentAccuracy\",\"executionTime\",\n",
    "                     \"motifSpots\",\"discoveredSpots\",\"DiscoveryLoops\"]\n",
    "experimentResults = pd.DataFrame(columns=experimentColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup\n",
    "\n",
    "1. Create a dataframe to store the experiment results\n",
    "2. Conduct the experiment for all fixed length files\n",
    "3. If variable length motifs are created, conduct the experiment for all variable length files\n",
    "\n",
    "Discover motifs for fixed length motif UI Logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, log in enumerate(UILogs):\n",
    "    file = pd.read_csv(folder_path + log)\n",
    "    insertSpots = UILogValidationDF.loc[UILogValidationDF['Filename'] == log][\"Index\"]\n",
    "    inserted_motif_spots = extract_numbers(insertSpots[insertSpots.index[0]])\n",
    "\n",
    "    window_sizes = [10,25,30,50]\n",
    "    # Encode the UI log using the function in util.py\n",
    "    uiLog = encoding_UiLog(file)\n",
    "    print(f\"UI Log {log} is encoded\")\n",
    "    for j, size in enumerate(window_sizes):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Split the string by underscores\n",
    "        # What does each position in the parts mean: 0: Log Type, 1: Sampling, 2: No. of Motifs, 3: No. of Occurrances, 4: Log Length, 5: Shuffle Percentage, 6: Reduction Percentage\n",
    "        parts = log.split('_')\n",
    "        experimentId = str(i)+\".\"+str(j)\n",
    "        new_row = {'experimentID': experimentId, 'uiLogName': log, \"variationPercentage\":  parts[5], \"motifsToBeDiscovered\": parts[2], \"numberOfOccurrancesToBeDiscovered\": parts[3],\n",
    "                    \"motifSpots\": inserted_motif_spots, \"windowSize\": size}\n",
    "        \n",
    "        inserted_motif_spots = extract_numbers(insertSpots[insertSpots.index[0]])\n",
    "        #Looping\n",
    "        k=0\n",
    "        insert_overlap_all = []\n",
    "        # Compare inserted values and discovered spots   \n",
    "        \n",
    "        while(k<5):\n",
    "            # Discovery motifs in the dataset\n",
    "            tm_matrix, event_series = discover_motifs(uiLog, size)\n",
    "\n",
    "            motif_idx_tm = np.argsort(tm_matrix[:, 0])[0]\n",
    "            nearest_neighbor_idx_tm = tm_matrix[motif_idx_tm, 1]\n",
    "            top_motifs = stumpy.motifs(T=event_series, P=tm_matrix[:,0], min_neighbors=1, max_matches=12)\n",
    "            # If not motifs are discovered in the first round, break and add this as information\n",
    "            if len(top_motifs[1][0]) == 0 and k == 0:\n",
    "                insert_overlap_all = \"No motifs discovered in the data\"\n",
    "                break\n",
    "        \n",
    "            insert_overlap, motif_overlap = compare_sets(set(inserted_motif_spots), set(top_motifs[1][0]), size)\n",
    "            insert_overlap_all = insert_overlap_all + insert_overlap\n",
    "\n",
    "            # Get indices for descending order by size (number of elements in each sub-array)\n",
    "\n",
    "            sorted_indices = top_motifs[1][0].argsort(axis=0)[::-1]\n",
    "            top_motifs_list = top_motifs[1][0][sorted_indices]\n",
    "\n",
    "            # Reduce all remaining indexes by the window size, Reduces only the indexes that are after (higher) then the identified motif, because lower indexes are not moved\n",
    "            # Does simulate a human checking the data and identifying equal processes\n",
    "            inserted_motif_spots = [item for item in inserted_motif_spots if item not in set(insert_overlap)]\n",
    "            for motif in motif_overlap:\n",
    "                uiLog = pd.concat([uiLog.iloc[:motif],uiLog.iloc[motif+size:]],ignore_index=True)\n",
    "                inserted_motif_spots = [(val - size) if val > motif else val for val in inserted_motif_spots]\n",
    "                # Break Criteria: We have done the discovery 5 times (k++) or we have found all motifs (set k = 5)\n",
    "                if len(inserted_motif_spots) == 0:\n",
    "                    k = 5\n",
    "                    break\n",
    "            \n",
    "            k+=1\n",
    "            new_row.update({\"DiscoveryLoops\": k})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        new_row.update({\"discoveredSpots\": insert_overlap_all,\"OccurancesDiscovered\": len(insert_overlap_all), \"executionTime\": end_time - start_time, \"windowSize\": size})\n",
    "\n",
    "        print(f\"Discovery for experiment {experimentId} with file {log} and window size {size} finished.\")\n",
    "        # Append the new row to the DataFrame\n",
    "        experimentResults = experimentResults._append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover motifs for variable length motifs in UI Logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if varLenDataAvailable:\n",
    "    for i, log in enumerate(varLenUILogs):\n",
    "        file = pd.read_csv(folder_path + log)\n",
    "        insertSpots = variableLenValidationDF.loc[variableLenValidationDF['Filename'] == log][\"Index\"]\n",
    "        inserted_motif_spots = extract_numbers(insertSpots[insertSpots.index[0]])\n",
    "\n",
    "        window_sizes = [10,25,30,50]\n",
    "        # Encode the UI log using the function in util.py\n",
    "        uiLog = encoding_UiLog(file)\n",
    "        print(f\"UI Log {log} is encoded\")\n",
    "        for j, size in enumerate(window_sizes):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Split the string by underscores\n",
    "            # What does each position in the parts mean: 0: Log Type, 1: Sampling, 2: No. of Motifs, 3: No. of Occurrances, 4: Log Length, 5: Shuffle Percentage, 6: Reduction Percentage\n",
    "            parts = log.split('_')\n",
    "            experimentId = str(i)+\".\"+str(j)\n",
    "            new_row = {'experimentID': experimentId, 'uiLogName': log, \"variationPercentage\":  parts[5], \"motifsToBeDiscovered\": parts[2], \"numberOfOccurrancesToBeDiscovered\": parts[3],\n",
    "                        \"motifSpots\": inserted_motif_spots, \"windowSize\": size}\n",
    "            \n",
    "            inserted_motif_spots = extract_numbers(insertSpots[insertSpots.index[0]])\n",
    "            #Looping\n",
    "            k=0\n",
    "            insert_overlap_all = []\n",
    "            # Compare inserted values and discovered spots   \n",
    "            \n",
    "            while(k<5):\n",
    "                # Discovery motifs in the dataset\n",
    "                tm_matrix, event_series = discover_motifs(uiLog, size)\n",
    "\n",
    "                motif_idx_tm = np.argsort(tm_matrix[:, 0])[0]\n",
    "                nearest_neighbor_idx_tm = tm_matrix[motif_idx_tm, 1]\n",
    "                top_motifs = stumpy.motifs(T=event_series, P=tm_matrix[:,0], min_neighbors=1, max_matches=12)\n",
    "                # If not motifs are discovered in the first round, break and add this as information\n",
    "                if len(top_motifs[1][0]) == 0 and k == 0:\n",
    "                    insert_overlap_all = \"No motifs discovered in the data\"\n",
    "                    break\n",
    "            \n",
    "                insert_overlap, motif_overlap = compare_sets(set(inserted_motif_spots), set(top_motifs[1][0]), size)\n",
    "                insert_overlap_all = insert_overlap_all + insert_overlap\n",
    "\n",
    "                # Get indices for descending order by size (number of elements in each sub-array)\n",
    "\n",
    "                sorted_indices = top_motifs[1][0].argsort(axis=0)[::-1]\n",
    "                top_motifs_list = top_motifs[1][0][sorted_indices]\n",
    "\n",
    "                # Reduce all remaining indexes by the window size, Reduces only the indexes that are after (higher) then the identified motif, because lower indexes are not moved\n",
    "                # Does simulate a human checking the data and identifying equal processes\n",
    "                inserted_motif_spots = [item for item in inserted_motif_spots if item not in set(insert_overlap)]\n",
    "                for motif in motif_overlap:\n",
    "                    uiLog = pd.concat([uiLog.iloc[:motif],uiLog.iloc[motif+size:]],ignore_index=True)\n",
    "                    inserted_motif_spots = [(val - size) if val > motif else val for val in inserted_motif_spots]\n",
    "                    # Break Criteria: We have done the discovery 5 times (k++) or we have found all motifs (set k = 5)\n",
    "                    if len(inserted_motif_spots) == 0:\n",
    "                        k = 5\n",
    "                        break\n",
    "                \n",
    "                k+=1\n",
    "                new_row.update({\"DiscoveryLoops\": k})\n",
    "            \n",
    "            end_time = time.time()\n",
    "            new_row.update({\"discoveredSpots\": insert_overlap_all,\"OccurancesDiscovered\": len(insert_overlap_all), \n",
    "                            \"executionTime\": end_time - start_time, \"windowSize\": size})\n",
    "\n",
    "            print(f\"Discovery for experiment {experimentId} with file {log} and window size {size} finished.\")\n",
    "            # Append the new row to the DataFrame\n",
    "            experimentResults = experimentResults._append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage Based Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI Log LenLog_1_1_10_10_10_1000.csv is encoded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_16852\\99399501.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  experimentResults = experimentResults._append(new_row, ignore_index=True)\n",
      "C:\\Users\\tomho\\AppData\\Local\\Temp\\ipykernel_16852\\99399501.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  experimentResults = experimentResults._append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI Log LenLog_1_1_10_10_1_10000.csv is encoded\n",
      "UI Log LenLog_1_1_10_10_2-5_4000.csv is encoded\n",
      "UI Log LenLog_1_1_10_10_5_2000.csv is encoded\n",
      "UI Log LenLog_1_1_10_15_10_1500.csv is encoded\n",
      "UI Log LenLog_1_1_10_15_1_15000.csv is encoded\n",
      "UI Log LenLog_1_1_10_15_2-5_6000.csv is encoded\n",
      "UI Log LenLog_1_1_10_15_5_3000.csv is encoded\n",
      "UI Log LenLog_1_1_10_20_10_2000.csv is encoded\n",
      "UI Log LenLog_1_1_10_20_1_20000.csv is encoded\n",
      "UI Log LenLog_1_1_10_20_2-5_8000.csv is encoded\n",
      "UI Log LenLog_1_1_10_20_5_4000.csv is encoded\n",
      "UI Log LenLog_1_1_10_25_10_2500.csv is encoded\n",
      "UI Log LenLog_1_1_10_25_1_25000.csv is encoded\n",
      "UI Log LenLog_1_1_10_25_2-5_10000.csv is encoded\n",
      "UI Log LenLog_1_1_10_25_5_5000.csv is encoded\n",
      "UI Log LenLog_1_1_10_5_10_500.csv is encoded\n",
      "UI Log LenLog_1_1_10_5_1_5000.csv is encoded\n",
      "UI Log LenLog_1_1_10_5_2-5_2000.csv is encoded\n",
      "UI Log LenLog_1_1_10_5_5_1000.csv is encoded\n",
      "UI Log LenLog_1_1_15_10_10_1500.csv is encoded\n",
      "UI Log LenLog_1_1_15_10_1_15000.csv is encoded\n",
      "UI Log LenLog_1_1_15_10_2-5_6000.csv is encoded\n",
      "UI Log LenLog_1_1_15_10_5_3000.csv is encoded\n",
      "UI Log LenLog_1_1_15_15_10_2250.csv is encoded\n",
      "UI Log LenLog_1_1_15_15_1_22500.csv is encoded\n",
      "UI Log LenLog_1_1_15_15_2-5_9000.csv is encoded\n",
      "UI Log LenLog_1_1_15_15_5_4500.csv is encoded\n",
      "UI Log LenLog_1_1_15_20_10_3000.csv is encoded\n",
      "UI Log LenLog_1_1_15_20_1_30000.csv is encoded\n",
      "UI Log LenLog_1_1_15_20_2-5_12000.csv is encoded\n",
      "UI Log LenLog_1_1_15_20_5_6000.csv is encoded\n",
      "UI Log LenLog_1_1_15_25_10_3750.csv is encoded\n",
      "UI Log LenLog_1_1_15_25_1_37500.csv is encoded\n",
      "UI Log LenLog_1_1_15_25_2-5_15000.csv is encoded\n",
      "UI Log LenLog_1_1_15_25_5_7500.csv is encoded\n",
      "UI Log LenLog_1_1_15_5_10_750.csv is encoded\n",
      "UI Log LenLog_1_1_15_5_1_7500.csv is encoded\n",
      "UI Log LenLog_1_1_15_5_2-5_3000.csv is encoded\n",
      "UI Log LenLog_1_1_15_5_5_1500.csv is encoded\n",
      "UI Log LenLog_1_1_20_10_10_2000.csv is encoded\n",
      "UI Log LenLog_1_1_20_10_1_20000.csv is encoded\n",
      "UI Log LenLog_1_1_20_10_2-5_8000.csv is encoded\n",
      "UI Log LenLog_1_1_20_10_5_4000.csv is encoded\n",
      "UI Log LenLog_1_1_20_15_10_3000.csv is encoded\n",
      "UI Log LenLog_1_1_20_15_1_30000.csv is encoded\n",
      "UI Log LenLog_1_1_20_15_2-5_12000.csv is encoded\n",
      "UI Log LenLog_1_1_20_15_5_6000.csv is encoded\n",
      "UI Log LenLog_1_1_20_20_10_4000.csv is encoded\n",
      "UI Log LenLog_1_1_20_20_1_40000.csv is encoded\n",
      "UI Log LenLog_1_1_20_20_2-5_16000.csv is encoded\n",
      "UI Log LenLog_1_1_20_20_5_8000.csv is encoded\n",
      "UI Log LenLog_1_1_20_25_10_5000.csv is encoded\n",
      "UI Log LenLog_1_1_20_25_1_50000.csv is encoded\n",
      "UI Log LenLog_1_1_20_25_2-5_20000.csv is encoded\n",
      "UI Log LenLog_1_1_20_25_5_10000.csv is encoded\n",
      "UI Log LenLog_1_1_20_5_10_1000.csv is encoded\n",
      "UI Log LenLog_1_1_20_5_1_10000.csv is encoded\n",
      "UI Log LenLog_1_1_20_5_2-5_4000.csv is encoded\n",
      "UI Log LenLog_1_1_20_5_5_2000.csv is encoded\n",
      "UI Log LenLog_1_1_30_10_10_3000.csv is encoded\n",
      "UI Log LenLog_1_1_30_10_1_30000.csv is encoded\n",
      "UI Log LenLog_1_1_30_10_2-5_12000.csv is encoded\n",
      "UI Log LenLog_1_1_30_10_5_6000.csv is encoded\n",
      "UI Log LenLog_1_1_30_15_10_4500.csv is encoded\n",
      "UI Log LenLog_1_1_30_15_1_45000.csv is encoded\n",
      "UI Log LenLog_1_1_30_15_2-5_18000.csv is encoded\n",
      "UI Log LenLog_1_1_30_15_5_9000.csv is encoded\n",
      "UI Log LenLog_1_1_30_20_10_6000.csv is encoded\n",
      "UI Log LenLog_1_1_30_20_1_60000.csv is encoded\n",
      "UI Log LenLog_1_1_30_20_2-5_24000.csv is encoded\n",
      "UI Log LenLog_1_1_30_20_5_12000.csv is encoded\n",
      "UI Log LenLog_1_1_30_25_10_7500.csv is encoded\n",
      "UI Log LenLog_1_1_30_25_1_75000.csv is encoded\n",
      "UI Log LenLog_1_1_30_25_2-5_30000.csv is encoded\n",
      "UI Log LenLog_1_1_30_25_5_15000.csv is encoded\n",
      "UI Log LenLog_1_1_30_5_10_1500.csv is encoded\n",
      "UI Log LenLog_1_1_30_5_1_15000.csv is encoded\n",
      "UI Log LenLog_1_1_30_5_2-5_6000.csv is encoded\n",
      "UI Log LenLog_1_1_30_5_5_3000.csv is encoded\n",
      "UI Log LenLog_1_1_60_10_10_6000.csv is encoded\n",
      "UI Log LenLog_1_1_60_10_1_60000.csv is encoded\n",
      "UI Log LenLog_1_1_60_10_2-5_24000.csv is encoded\n",
      "UI Log LenLog_1_1_60_10_5_12000.csv is encoded\n",
      "UI Log LenLog_1_1_60_15_10_9000.csv is encoded\n",
      "UI Log LenLog_1_1_60_15_1_90000.csv is encoded\n",
      "UI Log LenLog_1_1_60_15_2-5_36000.csv is encoded\n",
      "UI Log LenLog_1_1_60_15_5_18000.csv is encoded\n",
      "UI Log LenLog_1_1_60_20_10_12000.csv is encoded\n",
      "UI Log LenLog_1_1_60_20_1_120000.csv is encoded\n",
      "UI Log LenLog_1_1_60_20_2-5_48000.csv is encoded\n",
      "UI Log LenLog_1_1_60_20_5_24000.csv is encoded\n",
      "UI Log LenLog_1_1_60_25_10_15000.csv is encoded\n",
      "UI Log LenLog_1_1_60_25_1_150000.csv is encoded\n",
      "UI Log LenLog_1_1_60_25_2-5_60000.csv is encoded\n",
      "UI Log LenLog_1_1_60_25_5_30000.csv is encoded\n",
      "UI Log LenLog_1_1_60_5_10_3000.csv is encoded\n",
      "UI Log LenLog_1_1_60_5_1_30000.csv is encoded\n",
      "UI Log LenLog_1_1_60_5_2-5_12000.csv is encoded\n",
      "UI Log LenLog_1_1_60_5_5_6000.csv is encoded\n"
     ]
    }
   ],
   "source": [
    "window_sizes = [5,10,15,25,30,35]\n",
    "experimentResults = pd.DataFrame(columns=experimentColumns)\n",
    "\n",
    "for i, log in enumerate(percentageLogs):\n",
    "    file = pd.read_csv(folder_path + log)\n",
    "    # Getting the row with the uiLog name from the validation data\n",
    "    comparisonVariables = percentageValData.loc[percentageValData['uiLogName'] == log]\n",
    "    # Where the motifs were initially added\n",
    "    insertSpots = comparisonVariables[\"motifSpots\"]\n",
    "\n",
    "    # Encode the UI log\n",
    "    uiLog = encoding_UiLog(file)\n",
    "    print(f\"UI Log {log} is encoded\")\n",
    "\n",
    "    # Where are the motifs actually in the dataframe\n",
    "    inserted_motif_spots = extract_numbers(insertSpots[insertSpots.index[0]])\n",
    "\n",
    "    for j, size in enumerate(window_sizes):\n",
    "        start_time = time.time()\n",
    "        experimentId = str(i)+\".\"+str(j)\n",
    "        new_row = {'experimentID': experimentId, 'uiLogName': log, \"variationPercentage\": comparisonVariables[\"variationPercentage\"][comparisonVariables.index[0]], \n",
    "                   \"motifLength\": comparisonVariables[\"motifLength\"][comparisonVariables.index[0]], \n",
    "                   \"windowSize\": size, \"windowSizeMatch\": comparisonVariables[\"motifLength\"][comparisonVariables.index[0]]-size,\n",
    "                   \"percentageMotifsOverLog\": comparisonVariables[\"percentageMotifsOverLog\"][comparisonVariables.index[0]],\n",
    "                   \"motifsToBeDiscovered\": comparisonVariables[\"motifsToBeDiscovered\"][comparisonVariables.index[0]], \n",
    "                   \"motifsDiscovered\": 1, \"numberOfOccurrancesToBeDiscovered\": comparisonVariables[\"numberOfOccurrancesToBeDiscovered\"][comparisonVariables.index[0]],\n",
    "                    \"motifSpots\": inserted_motif_spots}\n",
    "        \n",
    "\n",
    "        # Discovery motifs in the dataset\n",
    "        tm_matrix = None\n",
    "        event_series = None\n",
    "        tm_matrix, event_series = discover_motifs(uiLog, size)\n",
    "\n",
    "        matches_match = True # Do While Construct with condition checked after each run\n",
    "        maximalNoOfMatches = 10 # Initiating by looking for 10 occurances\n",
    "        while matches_match: # As long as we have found the same amount of motifs as expected we increase the max match\n",
    "            top_motifs = None\n",
    "            top_motifs = stumpy.motifs(T=event_series, P=tm_matrix[:,0], min_neighbors=1, max_matches=maximalNoOfMatches)\n",
    "            insert_overlap, motif_overlap, overlapDF = compare_sets(set(inserted_motif_spots), set(top_motifs[1][0]), (size/2))\n",
    "            matches_match = (len(insert_overlap) >= (maximalNoOfMatches*0.8))\n",
    "            maximalNoOfMatches += 10\n",
    "\n",
    "        end_time = time.time()  \n",
    "        new_row.update({\"discoveredSpots\": motif_overlap, \"OccurancesDiscovered\": len(insert_overlap), \n",
    "                        \"executionTime\": end_time - start_time, \"windowSize\": size, \n",
    "                        \"DiscoveryPercentage\": len(insert_overlap) / comparisonVariables[\"motifsToBeDiscovered\"][comparisonVariables.index[0]] * 100,\n",
    "                        \"alignmentAccuracy\": overlapDF['alignmentAccuracy'].mean()})\n",
    "        experimentResults = experimentResults._append(new_row, ignore_index=True)\n",
    "\n",
    "experimentResults.to_csv(folder_path + \"experimentResults.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of single Event Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.98718044, 2.20448408, 2.21605981, 2.35616234,\n",
       "         2.46303571, 2.51940822, 2.61822832, 2.71273244, 2.74764111]]),\n",
       " array([[1308, 2009, 4575, 3163, 3266, 3782, 5215, 4211, 2413, 1584]],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv(folder_path + \"LenLog_1_1_30_5_2-5_6000.csv\")\n",
    "uiLog = encoding_UiLog(file, cooccuranceBased=True)\n",
    "\n",
    "size = 15\n",
    "#Discovery motifs in the dataset\n",
    "tm_matrix, event_series = discover_motifs(uiLog, size)\n",
    "\n",
    "motif_idx_tm = np.argsort(tm_matrix[:, 0])[0]\n",
    "nearest_neighbor_idx_tm = tm_matrix[motif_idx_tm, 1]\n",
    "top_motifs = stumpy.motifs(T=event_series, P=tm_matrix[:,0], min_neighbors=1, max_matches=10)\n",
    "top_motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4575, 4211]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste = [5950, 5695, 5471, 5127, 4969, 4829, 4685, 4645, 4577, 4213, 4201, 3835, 3685, 3674, 3470, 3323, 3125, 3029, 2921, 2645, 2567, 2280, 2167, 1660, 1300, 1172, 1094, 704, 693, 425]\n",
    "\n",
    "top_motifs = stumpy.motifs(T=event_series, P=tm_matrix[:,0], min_neighbors=1, max_matches=10)\n",
    "\n",
    "insert_overlap, motif_overlap, overlapDF = compare_sets(set(liste), set(top_motifs[1][0]), (size/2))\n",
    "print(overlapDF['alignmentAccuracy'].mean())\n",
    "motif_overlap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeSeriesData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
